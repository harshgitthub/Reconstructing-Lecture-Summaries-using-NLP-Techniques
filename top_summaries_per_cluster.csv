Cluster,cluster_rank,composite_score,cleaned_summary
0,1.0,0.7173882697290735,"we started with the equation yfx for fluid flowing in pipe where we have the values of the inputs ie flow values and output temperature earlier people used to derive the relationship between the inputs and the outputs empirically using mathematical formulations
but now we have many machine learning algorithms which can give us the relationship
1 slr  simple linear regression
2 mlr  multiple linear regression
3 logistic regression
4 random forest
5 kmeans clustering
6 hierarchical clustering

then we discussed about the 4 levels of measurement

1 nominal eg  gender color
    the data in this level of measurement can only be classified into different groups there is  
    no ordering and the data is discrete
2 ordinal eg  grades
    this is also discrete like nominal but has an inherent order associated with it

for working with the data we encode it by assigning the labels some values like male1 female2 etc which is fundamentally wrong as we are completely ignoring the values of the assigned numbers
instead we can use onehot code for the labels which acts as a switch making only a single value 1 and rest of them to be zero for example if we are classifying images of animals then for an image of a monkey the values will be dog0 cat0 monkey1 horse0

3 interval eg  temperature ph credit score
    the data in this level of measurement is continuous zero here is arbitrarily defined and  
    has no real meaning
4 ratio eg  height weight salary
    this is also continuous but here zero has a meaning in this level of measurement

both nominal and ordinal levels are discrete and they fall under the classification ml category whereas interval and ratio fall under the regression category

in the equation yfx y are the labels and x are the features

1 supervised learning when we have both labels and features it is called supervised 
     learning
2 unsupervised learning when we only have features and no labels it is called 
    unsupervised learning 
    in this we use the features to group data into clusters and then on the basis of their 
    similarities we can define their labels and then predict the correct group for any new x

at last we looked into the population for our data is too large that either we dont have access to all the populations data or we cant normally process such a large data
so we take samples from the population to get the data with the aim being getting as close as possible to the whole population"
0,2.0,0.706263543972741,"first of all we learnt about names of different types of techniques like slr which is simple linear regression multiple linear regression logistic regression random forest k means clustering and hierarchal clustering we learnt about their names only first whatever type of ml technique we use we will always get generic equations of type yb0b1x1b2x2
there are four different levels of measurement nominal ordinal interval and ratio we learnt about each of them in brief nominal like gender and colour we can only categorise them there is no ordering we can only create a count from them we cant calculate mean median mode measurements are always discrete in this category then comes the ordinal level of measurement for example grades aa  ab like thatthis is also for discrete values assigning numbers like 12 is wrong as b will have double value then a so we use encoding like 1and 0 giving values then we have the interval level of measurement which is used for measuring continuous values and it is used where zero has any arbitrary function like temperature the last is ratio which is again used for measuring continuous but in this zero has a meaning like height weight or salary then we learned that in y fx y is the label and x is called features supervised learning is when we have both labels and features available phenomenal and ordinal levels of we use classification and for interval and ratio level of measurement we use regression when why is not known then we use unsupervised learning under which comes k means clustering and hierarchical clustering like we have only x and which is spreaded into clusters and from that we try to predict y using some ml techniques the last thing we learned was the difference between sample and population sample is the data we have access to and population is the total amount of data and however the large amount of data we have access to it is always the sample for us and never the population"
0,3.0,0.6948580128313926,"so todays discussion start with two types of machine learning model which is supervised and unsupervised for example supervised ml includes  simple linear regression multiple linear regression random forest and unsupervised ml includes kmeans clustering and hierarchal clustering the most fascinating things is that whatever type of ml we will use always get a generic equation  yxb0  b1x  b2x2 
further ahead we have learnt about 4 levels of measurement 
1 nominal  it have discrete values and we can only categorize them also there is no ordering between them example  gender color
2 ordinal  it also have discrete values and work same as nominal level of measurement example  grades
3 interval  it have continuous value the concept of 0 is arbitrary in this case example  temperature
4 ratio  it also have continuous value in this measurement 0 has a meaning example  height weight salary
 in case of nominal and ordinal we use onehot encoding to change words into numbers by making vector
we learned that in y fx where y is the label and x is called features
when we have both labels and features then we use supervised learning and when we have only features then we use unsupervised learning method nominal and ordinal are use for classification purpose whereas interval and ratio are use for regression
second thing what we learnt today about data in ml we use sample instead of population
sample is a small chunk of population
thats what we have learnt today"
0,4.0,0.6886997840201443,"ds 203 15 january 2025 3rd lecture
sir started by talking about y  fx he first talked about old and new methods of data analysis for this he took the example of finding the temperature difference between the two ends of a pipe using the flow rate of the fluid flowing inside the pipe y was the temperature difference denoted by deltat and x was the flow rate
in the old methods we used to get the equation relating y  fx like dt  l23 while in the new methods we get data points and obtain a plot of the curve which relates temperature difference with the corresponding flow rate
then he talked about the methods that we use for obtaining the plot which are
1	slr  simple linear regression
2	mlr  multiple linear regression
3	logistic regression
4	random forest
he also mentioned kmeans clustering and hierarchical clustering
then he said that there are two paths which are machinelearning and the other one is statistics in this course we would frequently move from one path to another to get the assigned task done
he then started talking about levels of measurement there are four levels of measurement
1	nominal discrete for example gender color etc 
to explain in this if two people have different gender no one is superior the other all of them are equal
2	ordinal discrete for example grades
but now how would the computer identify these here gender as distinct because we need to associate some number with each grade for the computer to recognize them
but if we assign numbers like male 1 female2 and so on then we are doing something that is fundamentally wrong because we are attaching higher value to one of the them whereas they should be equal
in order to take care of this issue we use vectors and use them as a switch it would be difficult to show that it here but sir has drawn that very nicely in his notes uploaded on moodle
example dog 1 0 0 0
3	interval continuous for example temperature
sir said that scale on which we are measuring will not create any issues
4	ratio continuous for example height weight salary
y is known as label and x is known as feature
supervised and unsupervised learning problems
a problem in which we know both the values of the label and the features is known as supervised learning problem

then sir defined a function 
monthlypurchases  f salary month of year size of family etc
then he talked about the ml categories about each of the level of measurement 
nominal  classification
ordinal  classification
interval  regression
ratio  regression
then he talked about unsupervised learning in which we dont know the value of labels associated with features
there is hierarchical and kmeans clustering then he explained clustering by using a graph between features
it is difficult to explain the flow chart on page 9 in sirs notes but it is quite clear by looking at the flow chart

we take a representative sample out of the population in order to save time and money for data analysis
larger the value of the population more accurate is the prediction"
0,5.0,0.6840124099451199,"in this lecture we started by looking at how results were used to be predicted earlier without any ml models people used to collect data through experiments and observations and then by using those x and corresponding y values they used to manually fit the data into some curverelation which could then be used to predict future values
but with the advent of ml algorithms the need to manually determine relations has vanished and we can use various available ml models to fit relations among collected data and the model can predict any future values fed to it some of these algorithms include linear and multiple regression random forests etc
next we studied about the 4 levelsscales of measurement nominal ordinal interval and ratio
the nominal level categorizes the labels qualitatively into groups which dont have any specific order we can use frequency distributions to analyze this class of labels also since in nominal level we do not consider ordering it is incorrect to represent the values in terms of single numerical values instead we should use a vector which will have only one value as 1 and others 0 depending on which category the label y belongs to
next we talked about ordinal level which is associated with a specific sequence the example we considered that of grades grades have a specific order of importance but the interval between these is not fixeddefined
the interval level consists of values that have equal intervals but the absolute zero for the measurements is not defined in this level also order matters we discussed the example of temperature in celsius or fahrenheit scales in these scales the 0 value is not absolute so we can just compare the difference between two values of temperature and comment on which one is hotter but we cannot comment on the ratio of temperatures
ratio class has an absolute zero defined and at this value it means that there is complete absence of that measurable we considered example of height and weight here so heights of two persons can be compared and we can comment about the ratio of their heights eg we can say a person with height of 5 ft is twice as tall as the one with height of 25ft
nominal and ordinal levels consist of discrete data whereas interval and ratio levels contain continuous data
next we talked about supervised and unsupervised learning models
supervised leaning models are the ones in which we feed the complete data consisting of both the features and labels based on which the ml model determines the future values it uses classification or regression techniques for the same
in unsupervised learning only the x values are given based on the various x values features the machine learns by itself and makes clusters of values with similar features and assigns labels to each after which it can predict futures values based on the determined relations
we use kmeans clustering and hierarchical clustering algorithms for the same
so the ml models try to determine the function f which relates y and x as yfx based on available data
we can always collect a sample of the entire population of data to develop these models"
1,1.0,0.8114391753428541,"feature encoding techniques

during this session we reviewed several techniques used to transform categorical data into numeric representations which is an essential step in getting data ready for machine learning models the discussion began with the introduction to feature encoding techniques such as vectorization techniques and utilization of onehot encoding label encoding integer encoding binary encoding frequency encoding and target encoding

onehot encoding
onehot encoding converts categorical features into vector where each category has a separate column onehot encoding is particularly helpful when encoding the input features x in the case of multiclass or multilabel problems onehot encoding is less ideal for encoding target variables y one of the primary disadvantages of onehot encoding is that it can cause the curse of dimensionality for example if you have a variable such as pincode with thousands of distinct values applying onehot encoding will lead to a dimensionality explosion making the dataset sparse and computationally intensive

label encoding vs integer encoding
label encoding gives each category a specific integer the method is wellsuited to encoding target variables y especially for classification tasks but may not be suitable for input features where the model may mistake the numerical ordering as an ordinal relationship on the other hand integer encoding is used when the target variable is naturally ordinal instead of nominal such that the order of the categories holds important meaning

binary encoding
binary encoding or pseudo onehot encoding offers a more compact representation compared to onehot encoding by converting categorical values into binary code a few columns for example three columns can represent multiple classes up to eight classes in this example this method helps mitigate the issue of high dimensionality while preserving the distinctiveness of each category

frequency encoding
frequency encoding substitutes each class with its frequency in the dataset while it reduces the representation it is not necessarily good for target variables because two classes can have the same frequencies causing possible loss of useful information

target encoding
target encoding allocates to each category the target variables mean for that category target encoding can be especially useful when there is high correlation between the target variable and the categorical feature however caution needs to be exercised to prevent data leakage while training the model

simplification strategies
tackling complicated regression questions by converting them into classification questions using methods such as feature binning is common also introduced briefly was converting text into numeric vectors through vectorization methods as a key step in handling unstructured data

in general the suitable encoding method is determined by the type of data if the variable is utilized as a feature or target and possibly the effect it may have on dimensionality every approach has its strengths and weaknesses and the proper choice is critical to constructing efficient and effective machine learning models"
1,2.0,0.7879553409210145,feature encoding when either the dependent variable or some of the independent variables are categorical then they have to be appropriately encoded prior to being used for training ml models the project will be related to assessment of exercises which we have done label encoding one hot encoding binary encoding integer encoding frequency encoding target encoding multiclass problem mnist dataset there are 10 classes09 multilabel problem there are multiple labels associated with a single object a image with both cat and dog how we encode a variable depends on domain knowledge label encoding take categorical variables and assign numerical values to these valuesthis for nominal variable we can use label encoding to encode output variable but we should avoid using it on input variable integer encoding is for ordinal variable the numbers carry a value and have a meaning one hot encoding converting the output into a vector of dimension of number of classes this increases the number of columns in data and introduces the curse of dimensionality they choice of encoding depends on number of classes one hot encoding can be used for nominal variables and classes are not too many binary encoding it is just a different notation of one hot encoding we are converting vectors in one hot encoding into binary valuespseudo one hot encoding  frequency encoding the category values are replaced with its frequency in the column class takes on value of occurance we have to check whether two classes have same frequency target encoding we collect all y values corresponding to a particular class take their average and use this to represnt x in input this is used to encode the input variables for encoding input variables we can use one hot encoding generally for output variable encoding we can use other techniques eda is important as if we are not able to capture what data is properly we will not be able to perform the further processes correctly  we need to think multiple times before applying a particular encoding feature binning many times we want to convert a continous problem to a discrete problem we will divide the continous variable into bins and then assign categories to this this now becomes a classification problem it can be used for several reasons including problem simplification reducing the impact of outliers and noise in the data handling non linear relationships how to process text data natural language processingnlp examples of the manner in which statistical processing can generate deterministic output code generation how to convert text into numbers so that analysis is useful method1 drop the common wordsstop words convert to lower case  create a dictionary express the document using the dictionary in a sentence we cant know exact meaning until we know the context in which it is being used as each word might have multiple uses
1,3.0,0.7572374432958879,"today we started the discussion with feature encoding methods when either of the dependent or independent variables are categorical we have to use feature encoding techniques to convert them into numerical forms with which we can work with 
one hot encoding method creates columns which are equal to the number of unique categorical labels in the original data and fills each of these columns with respective true1 and false0  values this method should be used very cautiously as it increases the dimensionality of the data significantly and becomes cumbersome to work with for one hot encoding treated data the classification models should be tree based or neural network based as logistic regression cannot handle such data to overcome the curse of dimensionality of one hot encoding we can follow another method which is similar to onh but uses bits instead of true or false values to encode the categories for example for categories of weather related data like hot very hot the encodings can be 001 and 011 respectively other most widely used data encoding method is label encoding in this integers are allotted to different categories present in the columns this is best suited for nominal scale entries of the columns as in this method there is no ordering of the integer values that is for example red1 and blue2 does not mean that blue is greater than red and its also important to note that this method works well for y and using this method for encoding x features must be avoided integer encoding is another type of encoding which is very similar to label encoding but the only difference is the integer values have a sense of ordering and thus is suitable for ordinal scale valued columns frequency encoding uses the number of occurrences of  particular category in a column and sets as the label for that particular category should only be used for features and not target variables because there might be possible that two different categories might have same frequency but this issue some how doesnt affect the feature variables and thus can be used target encoding this also used for encoding the feature variables where for a particular category all the corresponding y values are noted and then average of these noted values is set as the category label we then discussed feature binning where the features with continuous values in discretised by making bins or categories so that the regression problem can be converted into classification problem  such conversion might be very useful when the actual data variance is very high and the r squared value of the regression model is very low began discussion about processing text and how the text data is converted into numerical data which can be used to do some useful analysis the common stop words are removed and dictionary of all the other words is made and a vector is associated to each sentence of the document based on whether the given word from the dictionary made is present in that particular sentence or not"
1,4.0,0.7518493385938712,"in this session we explored various feature encoding techniques essential for converting categorical and textual data into numerical representations suitable for machine learning models initially we discussed vectorization and onehot encoding vectorization is a general approach to transform textual or categorical data into numerical vectors onehot encoding specifically converts categorical variables into binary vectors creating separate columns for each category this method is widely used in multiclass and multilabel classification problems but can introduce the curse of dimensionality when dealing with variables having many unique categories

we then examined label encoding and integer encoding label encoding assigns each categorical class a unique integer value while this method is straightforward it implies an ordinal relationship between categories which may not always be appropriate integer encoding is particularly useful when the categorical variable represents ordinal data categories with a meaningful order as it preserves the inherent ordering

due to the limitations of onehot encoding such as increased dimensionality we introduced binary encoding compact encoding binary encoding efficiently represents multiple categories using fewer columnsfor example just three columns can encode up to eight distinct classes this approach helps mitigate dimensionality issues while retaining meaningful category distinctions

additionally we covered frequency encoding and target encoding frequency encoding involves assigning each category a numerical value based on its frequency within the dataset target encoding replaces categories with values derived from the target variable such as the mean target value for each category effectively capturing relationships between categories and the outcome variable

finally we briefly touched upon methods for converting textual data into numerical vectors through vectorization techniques setting the stage for deeper exploration in future sessions"
1,5.0,0.7475019181446368,"in todays class we covered different techniques for encoding categorical data we started with feature encoding where we learned about onehot encoding and vectorization onehot encoding is useful for categorical data but it can lead to the curse of dimensionality when the number of unique categories is very large vectorization on the other hand helps convert text data into numerical form which we only touched on briefly toward the end of the lecture  

we also explored label encoding which assigns a unique integer to each category however if the labels have a natural order making them ordinal rather than nominal using integer encoding would be more meaningful for multiclass and multilabel problems different encoding approaches may be required depending on the complexity of the data  

binary encoding was another method we discussed where each category is converted into a binary format which helps reduce dimensionality  for example three binary columns can represent up to eight classes we also learned about frequency encoding where the frequency of a category is used as its encoded value and target encoding where the encoding is based on the relationship between the category and the target variable  

overall the session introduced us to the challenges and tradeoffs involved in different encoding methods and gave us a basic idea of how to handle text data through vectorization"
2,1.0,0.8682418314735558,"during exploratory data analysis we get insights on type of data and understand problems present in the data we can remove outliers based on trend followed by data if all data follows a particular trend and some doesnt follow we can remove this data we can use data smoothening to reduce noise in data real data has a lot of fluctuations this can make it difficult to find trends ir pattern in such data simple moving average sma consider a window around every data points and average the values window width can be varied to adjust the level of smoothening based on application we can take window width we can take moving average at a point as average of all past points or average of some past and some future points there will be problems at the end of data as there wont be any points to the past of starting of data so we have to define accordingly at end points the moving average reduce the noise and maintains the characteristics data another way is exponential moving average that weight nearby samples more while averaging this is used in time series forecasting
in data analysis it is crucial to handle missing values and outliers before proceeding with further steps like calculating moving averages ignoring these aspects can lead to distorted results such as sudden changes or incorrect trend patterns handling missing values can be done by either ignoring discarding or using methods like regression to fill in the gaps outliers should be removed or adjusted to prevent them from skewing the analysis after addressing these issues the data is ready for further analysis where both visual and mathematical methods are used to ensure accuracy normalization transforms data to a 01 range in this case normalization had little effect on the target values and coefficients with minimal changes except for one variable x2 this shows that scaling didnt significantly impact the model for the next messy dataset a clustering algorithm is suggested in the process of data analysis independent normalization of variables x and y can impact the results especially when using algorithms like kmeans clustering which rely on euclidean distance when the data is normalized it alters the shape and relationships between variables leading to more refined results algorithms sensitive to scaling such as kmeans are particularly affected and normalization helps produce more accurate clusters by balancing the influence of different features


a standard normal distribution has a mean of 0 and variance of 1 to standardize data the formula is used where is an observation is the mean and is the standard deviation standardization does not change the shape of the datas distribution meaning the transformed data retains the original distributions shape this method is often used in statistical tests especially hypothesis testing where normal distribution is required data transformations like boxcox are applied to achieve normality before performing such tests descriptive statistics of the transformations should be reviewed to understand their impact
the boxcox transformation is a technique used to make data more normally distributed it involves raising each observation to a power lambda with the optimal lambda value determined through a process called maximum likelihood estimation in this case the lambda value is 017 which is chosen to best transform the data into a normal distribution this method is useful when algorithms assume normality in the data ensuring that the data fits those assumptions
the boxcox transformation and similar techniques like square root or cube root transformations pull data points closer together particularly those that are far from the mean this helps in dealing with skewed distributions for a lambda value close to 0 in boxcox the transformation approximates  if lambda is 0 it uses the log transformation the result is a more normally distributed dataset which is essential for certain algorithms when applying transformations like boxcox or logarithmic transformations to features eg x1 these same transformations must be applied consistently to future or test data to ensure accurate predictions additionally sometimes transformations must be reversed to interpret results in their original context various scaling methods such as standardization and normalization are commonly used to handle such transformations
the condition where data variance changes across different levels of the data is known as heteroscedasticity
the process of scaling data such as using log transformations reduces the emphasis on large values and minimizes the impact of variations in data this is crucial because many algorithms especially those that rely on euclidean distance or hierarchical clustering are sensitive to data scaling various scaling methods are used to bring the data to a common scale while preserving the variation within the dataset however in some cases transformations like the boxcox transformation are applied to also change the shape of the data for better analysis
when preparing data for analysis steps include identifying and fixing missing values exploring correlations between features and creating visual representations like scatter plots or matrix plots scaling transformations are often applied before conducting these analyses to ensure accurate and consistent results
the issue of data imbalance arises when certain classes are underrepresented in a dataset compared to other classes this is particularly problematic in classification tasks in the example given there are four classes red green blue and white where the white class is underrepresented and often merged with the blue class when subjecting these classes to classification boundaries are created by the algorithm based on the dominant class leading to misclassification of the underrepresented class
as a result false negatives occur where observations belonging to the underrepresented class are wrongly classified as another class this is evident in confusion matrices and the performance metrics like precision and recall where the underrepresented class shows poor classification the algorithm focuses on correctly classifying the majority class but the minority class suffers from poor performance
data imbalance occurs when one class significantly dominates over others in a dataset causing challenges for learning algorithms this imbalance can negatively affect the algorithms ability to learn particularly in cases like medical diagnosis eg diabetes detection or fraud detection where underrepresented classes are crucial algorithms may become biased toward the majority class leading to misclassification and poor performance for minority classes
an example of this is in the detection of exoplanets based on flux values from distant star systems most stars do not have planets meaning the dataset is heavily skewed with 993 representing stars without planets and only 07 representing stars with planets in this case the algorithm may focus on the majority class stars without planets and miss important patterns in the minority class
to address this data needs to be balanced or projected into a lowerdimensional space for better visualization and understanding algorithms require different performance metrics eg precision recall to detect the presence of data imbalance and ensure that the model learns effectively from both the majority and minority classes neglecting this can lead to biased models that perform poorly on critical underrepresented classes

in cases of data imbalance particularly when the disparity between majority and minority classes is not extreme dropping a small number of values from the majority class can be acceptable without significantly affecting the samples representativeness however if the difference is significant dropping too many values from the majority class may lead to a sample that is no longer representative of the population
one approach to address data imbalance is oversampling the minority class by duplicating its data points however this method does not introduce new information it merely replicates existing data which can water down the dataset without solving the core issue of imbalance
a more effective method is using synthetic minority oversampling technique smote instead of duplicating data points smote generates synthetic samples by selecting a minority class data point and creating new points as linear interpolations between it and its nearest neighbors this method enhances the representation of the minority class while maintaining diversity in the dataset by creating synthetic samples smote helps improve the balance without simply duplicating existing data providing a better way to handle data imbalance in machine learning tasks
some other data balancing are adasyn tomek links are used to to undersample data majority class with their nearest neighbor being a minority class sample are removed smote and tomek links are done one after other to get nice data"
2,2.0,0.7476642635170009,"in todays class we move on to data smoothening using moving averages in the practical datasets we see many fluctuations due to detrimental noise which can lead to difficulties in obtaining trends and patterns in the dataset to handle the missing values in the dataset for example in stock analysis we dont have data of the upcoming days but we can fill the data using moving averages in the simple moving averagesma method we need to consider a window around the missing data point and average the values for this you can select the points from both sides of the point in the window or points from the left side of the point for points which are appearing after endpoints we use the final endpoints more than once after performing sma we can create the plot again the plot will be refined based on the window size chosen by us if we select a very large window size the plot nearly becomes a straight line so we need to choose the width of the window based on the data for better analysis we also have better methods like exponential moving averages that weigh nearby samples than moving averages if in a dataset one particular feature columns features have a larger magnitude compared to other features then we apply normalization to this column but due to this change only the coefficient of this column in the mlr model changes clustering methods based on distance can be highly influenced if one column has larger magnitudes or more variance than others then we saw the difference between normalization and standardization in normalization the data is transformed as xnxxminxmaxxmin while in standardization the data is transformed to a mean of 0 and standard deviation of 1 by the transformation 
znxmeanstd dev in which the mean and std dev are of the sample then we learned about the boxcox transformation in which we transform the original x to remove the skewness from the data and remove the situation of heteroskedasticity meaning the variance is changing in the data it is based on the maximum likelihood estimation technique then we saw the confusion matrix of a cluster plot in which a particular class was creating data imbalance data imbalance means certain class shows more frequency than others we need to handle this else we will get misleading results as an algorithm trained on imbalanced data may show biases toward a class techniques handling data imbalance are either to undersample the majority class or oversample the minority class a better approach is to generate synthetic data for the minority classoversampling we do this by performing a technique called smote in which we perform linear interpolation between existing samples smote is used to oversample the minority class while the technique of tomek links on the other hand undersample the data majority class samples with the nearest neighbours as minority class samples are removed from the data"
2,3.0,0.7227082283372226,"we continued our discussions on eda eda is the first step before performing any complex algorithms on the available data it involves discovering problems associated with the data as well as few possibilities and insights from the data in todays class we looked at some specific problems associated with the raw data and the methods algorithms used to tackle these
the first problem which we discussed was that of presence of lots of noise in the data which causes hindrance in detecting the true signal we discussed few methods to solve this problem we specifically looked at moving averages for data smoothening in which we first learnt about the simple moving average in this method we take nearly about 50 data points surrounding a particular data point and evaluate their average inorder to get an average value associated with that particular point we do the same with all the other points and finally get a smoother curve with the values as averages of some 5060 points in the neighborhood the smoothness of the curve depends on the number of data points we are choosing to evaluate the average as we increase this number the curve becomes smoother and smoother and eventually becomes a flat horizontal line when all the points in the set are included in the window this creates an artificial signal and the original variations in the data vanish completely  there can also be different ways in which we consider these points for example we may consider only those points which lie behind the selected point this causes a problem at the end point particularly the one on the left similarly we can also consider that window which includes some points behind the selected point and some ahead of it this also causes problem at the end points we can choose any of this however our choice should be justifiable according to the particular data set 
next we can also use exponential weighted averages in which we assign higher weights to the points in the proximity we must select an optimal size of window or the moving average method such that we are able to extract the signal from the highly fluctuating values 
before creating moving averages we should address the problems of missing values and outliers else they would cause trouble while creating the ma 
the next problem is that some values in a column have significantly higher values than some others this makes the gradient decent algorithm more biased towards the larger values thereby causing data imbalance and giving false results hence it is important to normalize the values in the columns so that every value lies between 0 and 1 standardization is another process wherein we convert the existing data into standard normal distributions with mean 0 and standard deviation 1 both of these do not change the shape of the data if we transform or scale the data then any algorithm based on calculating euclidean distances would be largely affected it is important to transform the data first because some algorithms assume that the data is already normally distributed 
there is another kind of transformation box cox transformation in which we evaluate the transformed value of each x by using a parameter lambda which is optimized such that we get the closest approximation to the normally distributed data set 
apart from transforming the features we also have to reverse transform the transform the response variables to get the values in the original form back 
the third problem which we discussed was regarding the data imbalance which occurs whenever we have a class whose number of samples are very small compared to the other classes hence the class is eventually suppressed by the others we need to fix this problem as we may get incorrect results predictions on using the imbalanced data also the algorithm may not learn well using the imbalanced data 
to fix this data imbalance we can do the following
1	under sample the majority class
2	over sample the minority class the most naive method is to duplicate the values of the existing points
3	ideally we should sample more points in the surrounding of these points by linearly interpolating between any two points this is known as smote
4	next instead of just randomly dropping any sample value from the majority class we can drop those values which have the nearest sample belonging from the minor class this has two advantages first is that it creates a distinct boundary between the classes and also in this way we can get rid of some possibly misclassified points this is known as tomek links
5	so first we can apply smote then use tomek links to improve the quality of the classification model"
2,4.0,0.7142824308297077,"todays class start with a discussion on how do we eliminate noise from dataset noise makes it difficult to find a trend in data therefore it is becomes a necessary part of exploratory data analysis we learn one method named simple moving average in which we consider a window around every data point and average the values window width can be varied to adjust the level of smoothing higher window size makes more smoothing this method is also used for filling up missing values replacing outliers
another method is exponential moving average which works better on time series data one good practice is that first removes the outliers then apply moving averages method next we learnt about the standardization and normalization of datasets linear regression model is immune to the data scaling but many models such as gradient descent method is not clustering algorithm based on euclidean distance will greatly influence by scaling the data the normalization makes value lie between 01 using formula such as xxxmin xmaxxmin and the standardization creates standard normal distribution normalization does not change the shape of distribution of the data we use log transformation when data is heteroscadascity next we discuss about data imbalance which happens when certain instances of a class might show up more frequency than others example rare disease diagnostic forgery we can overcome this by either under sampling the majority class or over sampling the minority class we can do this with the help of smote tool"
2,5.0,0.7128529126764622,"we started off the class by discussing about outliers and how they can be found out using scatter plots and box plots they can also be found out by descriptive statistics and line charts however whether to actually ignore and drop the outliers or to perform some processing on them depends upon the domain and thus required domain knowledge sometimes outliers can be hidden in the data which can be observed by maybe rescaling the data or by dropping the true outliers and then observing the remaining data again now we need to smoothen out our data and remove all the noise in order to observe some trends in the actual data thus we perform data smothering which could be done by various methods we discussed about simple moving averages where we consider a window around every data point and average the values in that window the window width can be adjusted according to the level of smoothening but in doing so we need to take care about missing values as well or else our smoothening algorithm might not work correctly the higher the window size the smoother our data becomes we also have methods called exponential moving average or weighted moving average which weighs the nearer points more as compared to the farther points 
we then moved on to handling data where each column has a different scale suppose we have a data where one column has very large values as compared to another column this becomes a problem when we try to run a model like multiple regression as the larger value data creates a very large maxima which leads to all the other minimas getting overshadowed hence gradient descent is not able to find the most optimum minima and the algorithm fails hence we need to scale our data appropriately to prevent this the most common method of scaling is to scale each column data to lie between 0 and 1 we then discussed that any algorithm which depends upon euclidean distances like the kmeans clustering algorithm will be affected by the normalisation of the data we could also perform an operation known as standardisation which comes from the term standard normal distribution ie n 0 1 standardisation and normalisation do not change the shape of the distribution of the data
the difference between the mean and the median of the data also gives us an idea about the skewness of the data where a larger difference indicates more skewness 
we then discussed about various other transformations like logarithmic transformation where we deemphasise the higher values this transformation actually changes the shape of the distribution of the data 
we then moved on to data imbalances which is mainly used in the context of classification it means that data from one class is highly underrepresented as compared to the other classes in such cases we can either undersample the majority class or generate data belonging to the minority class we could use smote algorithm which creates synthetic samples based on linear interpolation between the existing samples the hyper parameters for this algorithm are the number of neighbours we want and the smote percentage we also have tomek links which under sample the majority class data"
3,1.0,0.7194613901645228,"the lecture continued on the lines of logistic regression and classification ideas from where we left off we started off by discussing the error metrics and the confusion matrix then we were given a demonstration on some sample data on playgroundtensorfloworg which allowed us to play around with data and neural networks it allowed us to add features to our classification problem then we moved on to some code for logistic regression we understood that the score function for a logistic regression model library gives us the accuracy which we should not believe we also plotted the confusion matrix for the data and understood the true positives and true negatives we then studied the reciever operating characteristics roc curve the roc curve rose vertically at the start and then became horizontal this should us that for the given dataset our model first detected all the true positives before starting to detect false positives ideally the roc curve is usually shaped in a crescent shape the roc curve indicates the quality of the classifier where a sharper roc curve indicates a better model the logisticregression function returns the predicted class for an observation or the probability value associated with it hence by changing the threshold between the classes we can change the quality of the model and check it using the roc curve the 45 degree line on the roc curve tells us that there is no classification between the points hence the flatter the roc curve for our classifier the worse is our classification model we also measure the area under the roc curve and call it as auc for a good classifier this value is close to 1 the worst classifier is the one with the roc curve the same as the 45 degrees line hence the worst case auc value is 05 the points on the roc curve correspond to different threshold values for the classification 
we then moved on to data having more than 2 classes and we realised that sometimes due to lack of anomalous data we might not be able to use our classifier in the entire system of the classifier some classes may have very low f1score which means that they are underrepresented and the classifier cant be relied on for classification of data into that class each class has its own roc curve and hence a classifier can be good for one class while being equally bad for another which can be seen using the roc curve of that class for our classifier 
then we moved on to clustering which was a part of unsupervised learning in this case our data does not have any labels and we need to assign labels to the data ourselves based on some characteristics this assignment becomes a part of eda we then studied about some clustering algorithms like hierarchical clustering and kmeans clustering for kmeans clustering we need to give the number of clusters as the input which is difficult to find out just based on visualisation of data however in hierarchical clustering we plot a dendogram and then based on that we can decide the number of clusters we need 
in kmeans clustering we first specify the number of desired clusters then the algorithm starts randomly assigning points to any cluster and then we find the centroid of each cluster then we find the distance of each point in a cluster to the centroid of all the clusters then we reassign each point based on the centroid they are closest to this process repeats until each point is closest to the centroid of its cluster than any other cluster it is an on2 algorithm in this algorithm the initial assignment also changes our final results hence we run the algorithm multiple times and check that for each point which set was it clustered into multiple times and hence we decide the final cluster it belongs to 
hierarchical clustering does not require any initial cluster number it results in a dendogram which can help us decide our degree of clustering it starts by assigning each point as a different cluster it then gets grouped with other clusters and we get the dendogram the number of clusters can be decided by where we observe the dendogram the pair wise distance between each pair of clusters is calculated and those clusters are grouped whose distance is less than all the other pairwise distances of those clusters this goes on till we have just one cluster giving us the dendogram and the number of clusters is decided by where we cut the graph"
3,2.0,0.7164721053243975,"in the lecture we began by exploring a website called playgroundtensorfloworg which allows users to interactively experiment with machine learning models on this site we could select a dataset choose features and configure the number and size of hidden layers in the neural network by pressing the run button we could start training the model and observe how each neuron affects the final classification this provided a clear visual understanding of machine learning processes and helped us build an intuition for model behavior an interesting point was that even with a single hidden layer and appropriate features we were able to achieve high classification accuracy on challenging datasets

next we discussed logistic regression in the example code we didnt implement a traintest split which is something we should always do to evaluate the models performance more reliably we used pythons logistic regression library to train the model and evaluated it using the score function which in the case of logistic regression refers to the models accuracy unlike in linear regression where it corresponds to the rsquared value 

we also covered the receiver operating characteristic roc curve which is a graphical representation of a models performance the roc curve plots the true positive rate tpr against the false positive rate fpr a good classifier will classify most examples correctly before starting to make mistakes which results in a steep increase in tpr with a minimal increase in fpr we explained that for two classes we can assume 2d gaussian distributions each with different means when the distributions are well separated the overlap is minimal leading to fewer misclassifications however when the distributions are less well separated theres more overlap increasing the chance of classification errors the model calculates the probability of each classification and by adjusting the threshold we can compute different tpr and fpr values which form the roc curve if the area under the roc curve is 1 the classifier is considered perfect

following this we explored multiclass classification in this case we calculated the precision recall and f1 score for each class if one of the classes showed particularly low precision recall or f1 score it suggested that the dataset was imbalanced with too few instances of that class we also generated multiple roc curves one for each class to evaluate the performance across all categories

the lecture then shifted to clustering a form of unsupervised learning where we only have feature data x and no target variable y we learned about key clustering algorithms such as kmeans and hierarchical clustering and the importance of clustering metrics to evaluate how well the data points are grouped

in kmeans clustering the number of clusters k must be specified beforehand the algorithm starts by randomly selecting k initial cluster centers each data point is then assigned to the nearest center forming the initial clusters the centers are recalculated as the mean of the points in each cluster and the process repeats iteratively until the cluster centers no longer change indicating convergence

hierarchical clustering on the other hand does not require predefining the number of clusters instead it focuses on the distance between clusters using a method called linkage there are several types of linkage such as single complete average and wards method which define how the distance between clusters is measured in hierarchical clustering the process begins with each data point as its own individual cluster the two clusters with the smallest distance between them are merged and this process continues iteratively eventually all the points are combined into a single cluster a key feature of hierarchical clustering is that it can be represented visually through a dendrogram a treelike diagram that shows the sequence of merges by cutting the dendrogram at different levels we can obtain different numbers of clusters and visually inspect how the data points are grouped this gives us the flexibility to explore clusters at various levels of granularity

in conclusion the lecture covered foundational concepts in machine learning including model evaluation through accuracy and roc curves as well as clustering techniques like kmeans and hierarchical clustering these topics laid the groundwork for understanding how unsupervised and supervised learning methods work in practice"
3,3.0,0.709027351504871,"we first revised a few concepts from the previous class to assess the quality of any classification model we use the confusion matrix the confusion matrix gives us the number of points that are correctly identified and those that are incorrectly identified in a tabulated manner sir showed us some code for fitting a given data set with a logistic regression model and obtaining the various metrics associated with it the score represents the accuracy in a classification model just like it represents r in regression we also found out the true positive rate and the false positive rate these are key metrics used to evaluate the performance of a classification model true positive rate measures the fraction of the actual positives that are correctly determined by the model similarly false positives represent the proportion of actual negatives that are incorrectly classified as positive
the receiver operating curve roc plots tpr vs fpr if the curve is nearly vertical then it means before classifying any points as false positives all the true positives have already been identified on the other hand if the line is exactly 45 degrees it means the classification is completely random or we can say that we are not using any classifier at all so ideally we want our plot to be as steep as possible also the area under the curve for a good classifier should be as close to 1 as possible various points along the curve represent different threshold values as you increase the threshold the model becomes more stricter and hence there are lesser chances of obtaining true positives thereby increasing the false negatives 
a good classifier is fine until there arent any significant overlaps in the data sets however if the two sets have significantly high overlap then no classifier will work well for it
so we can improve the quality of our classifier by either transforming the data or creating new and relevant features using feature engineering
we also looked at yet another metric support this represents the total number of observations that are in support of the class ie the number of observations that have been classified under that class by the model
next we started with clustering this is another unsupervised learning algorithm in this the points in a given data set are clustered and different groups are made among these this is usually a part of eda and is used to assign labels to unlabeled data there are two types of models in it k means clustering and hierarchical clustering 
in k means clustering we have to specify the number of means or in other words the number of clusters that we want this is not the case for hierarchical clustering 
in k means clustering all the points are randomly classified first and based on these a mean value is obtained using this mean value the distance between a point and the means of various clusters is calculated and this is done for every point the point is reassigned to a cluster whose mean value is closest to the point the new mean for a cluster is re calculated based on the new points then again the points are reassigned into different clusters this is repeatedly done until we reach a step wherein there is no change in the arrangement of the points this is our final clustering 
if the initial assignment of the points to various clusters differs we can get different clusters every time for the same data set so we run multiple such algorithms and the final cluster for a point is chosen to be that cluster in which it falls for the maximum number of times 
hierarchical clustering algorithm considers every point as a cluster in itself and starts clubbing all the clusters one by one until it reaches a final single cluster with all the points this is represented as a dendrogram the height of the leg of the diagram represents the distance between the two pointsclusters so hierarchical clustering gives many different clusters which are combined and recombined to form larger and larger clusters we can decide upto which point we want the clusters thats it for this class"
3,4.0,0.6727588492751362,"in todays class we visit a website named playgroundtensorfloworg in which we play among the number of layers or neurons and features to see what kind of classification does it do next we see some codes related to logistic regression we see that we cant rely on just score or accuracy of the model we have to compare the elements of confusion matrices to see whether it is good model or not in the code we see two new terms 1 true positive rate  it is calculated as tp  tp  fn where tp is the number of true positive and fn is the number of false negative instances
2false positive rate  it is calculated as fpfptn where fp is the number of false positives and tn is the number of true negatives
next we learnt about the receiver operating characteristics curve roc  it tell us that when does the classifier start detecting incorrect false positive it refers the quality of classifier for a good classifier the curve should be straight if the curve gets flatter then it meant that the classifier is randomly classifying without any mathematical function
we would like a classifier that discriminates between the classes even if it is overlapping the other part of roc curve is area under curveauc it should be close to 1 for a good classifier if it is close to 05 then it is not a good classifier next we see an example of an imbalanced datasets where we calculate precision recall value for class 3 other than the precision and recall support represents the number of actual occurrences of a class in a datasets examples of imbalance datasets is fraudulent or scams interesting things is that we can convert regression problem into classification problem by creating bins but vice versa is not possible as it cause data losses we see the unsupervised method of classification which is clustering kmeans and hierarchical clusterings are two which we discuss in kmeans we have to specify the number of clusters but in hierarchical clustering we dont have to specify such things in hierarchical clustering every data points is assigned to its own unique cluster"
3,5.0,0.6631324937504723,in todays class we first started with a website named playgroundtensorfloworg in which we had several options of several features and different kinds of data types and we came to our conclusion that by increasing the number of features we can even use the simplest of the model to get the appropriate results that is the correct model delhi came to know about true positive rate and false positive rate and their formulae then we discussed the receiver operating characteristic curve which shows when does the classifier start detecting incorrect or the false positives like if the curve is steeply rising in the starting and then when it completely rows then it is turning towards the right then it is accurate rather than when it is less steeply rising or rising like a curve if we add more features roc will get better this is called feature engineering auc is the area under the roc curve and when auc is 5 this means that there is no classifier or there is no need of any classifier because it is something like tossing a coin we used to do like if probability is greater than equal to 05 then it is class one otherwise class zero so what happens if we change this probability to 04 or 03 so for that the different points on the roc curve correspond to these various different thresholds then using a simple data set we calculated precision and recall of different classes we can convert a regression problem to a classification problem like if we have heights we can create bins like 2  4 feet   4  6 feet like that but classification to regression is not possible because data loss happens in such a case when we started with the clustering which is a kind of unsupervised learning which means that we dont have y value we only have x value we then learned about kings clustering in which we pre decide the number of clusters and how this mechanism works then we learned hierarchical clustering in which there is no need to specify the number of clusters and which results in dendrogram
4,1.0,0.762341627606205,"in todays lecture we saw some more methods to reduce the dimensionality of data inorder to solve data problems associated with large dimensions 
in the last class we had talked about vif analysis we continued the discussion on it 
in vif analysis we start eliminating the features one by one with the ones having large vif values we continue the process until we get a set of features which have vif values smaller than the desired threshold which can be 105 we eliminate the features based on our domain knowledge
depending on the threshold set we can have different combination and numbers of features selected for the regression model in such a case we should evaluate the metrics for each of the model and then by comparing these we can decide which one to use
the next method we saw was that of pca principal component analysis it helps to reduce the dimensionality of the problem by creating orthogonal principal components each of the principal component is a combination of the original features with the weights called as loadings the loadings tell us the importance of each feature so instead of using too many features we are combining them into a principal component and using this to predict the response variable the number of principal components is equal to that of the original dimensions however we can decide to take up twothree of these pcs suitable to our model the first pc explains the maximum variance in the data the next explains some amount of the remaining variance this continues until all the principal components explain the complete variance in the data
the cumulative variance explained by all the pcs is given by the elbow diagram which is asymptotic to 1 the elbow diagram tells us how many pcs would be needed to explain a certain amount of variance in the data
among the two methods vif is generally preferred first because of interpretability x1x2x3 etc  represent the actual physical features whereas the pcs which are a combination of these are just mathematical parameters 
so pcs are most suitable for prediction analysis but they cannot be used for whatif analysis or delta analysis hence through pcs we achieve the goal of reducing the dimensions however we cannot do any sensitivity analysis
so normally vif is done first and if still there are many features left we perform pca
the advantages of using pca is that it helps in dimension reduction data reduction predictive analysis visualization understanding of the structure of the data so this is a part of eda and can be used to determine the number of clusters k in kmeans clustering
another disadvantage of pca is that it is sensitive to data scale hence normalization if required must be done before doing pca
next we talked about the tsne plots 
the tsne plots also reduce the large number of dimensions in the data into 23 dimensions which can be easily visualized this is done on the basis of the distance between the points in the ndimensional space through this method we lose the exactness of the data but instead we get the information about the relative closeness of the points 
tsne creates a probability distribution for each of the point which contains the probability of closeness of that point from every other point 
gaussian normal distribution is used in higher dimensional space but in tsne as the name suggests tdistribution is used tdistribution increases the distance between dissimilar points so in ndimensional space if the probability of two points being close is very high they are clustered together in the tsne plots
all these methods help in reducing the number of dimensions in the data set"
4,2.0,0.7533167576930839,"we continued with the correlation problems we observed the heat maps for perfectly corelated data and partially correlated data heatmaps are more coherent if they are created after sorting the data when we calculate vif we do not process the values of y but only xvalues vif is simply the function of x fxi  j in this method we progressively eliminate the features based on threshold vif we have to decide on the threshold value of r2 because we only choose the vif values below the threshold we have to check if the feature being eliminated is important for the data or not if it is important we cant just eliminate it 
lesson dont do think mechanically 
principal component analysis principal components are necessarily orthogonal to each other number of principal components is equal number of dimensions of the data out of these principal components we have to decide which among these are important pc1 is the axis that captures the best variance in the data pc2 is the second best axis to capture variance pca actually helps us to reduce the dimensionality of the data we plot the fraction of variance for these components these plots are called elbow plot each principal component is a mixture of features multiplied by loading but do not blindly follow the process first check if the dimensionality should need to be reduced or not in general vif needs to be done first because intercollinearity is a big problem 
after pca instead of representing y as a function of x we represent it in the form of function of pca after representing we do whatif analysis on the function we later explain the results however with pca we lose explainability we cant do any sensitive analyses vif reduced the number of features in real space and if we still have many methods we can go for pca 
pca helps in dimension reduction helps predicting model helps in visualization of data when pca is used for visualization which comes under eda pca presupposes normalization in the data we need to normalize the data before using pca we then tried these method on the data from midsem and noticed that out of 24 18 features were needed to explain the data pca is a lossless method data is not lost 
we started with tsne tdistributed stochastic neighbor encoding t stands for tdistribution probability wise tdistribution predicts more probability for wide spread data gaussian on the other hand is tighter it is stochastic in nature and gradient descent is involved in the process hence while running its code it is likely to take time here we lose the exactness of data but gain the closeness of data we can map multidimensional data onto lower dimensional data we usually do not prefer to plot beyond 3 dimensions"
4,3.0,0.7513632719628592,"vif variance inflation factor
purpose vif is used to detect multicollinearity in regression models
process we progressively removed values of ei likely referring to certain features or variables
outcome the test was conducted in code 2 and it helped in removing extra features

bca principal component analysis
principal components pc
  principal components are orthogonal meaning they are uncorrelated
  the maximum number of components is equal to the number of dimensions in the original dataset

variance explained by components
  the first principal component pc1 explains most of the variance in the data
  the second principal component pc2 explains the next largest portion of the variance and so on

loadings
  if pc1 is a linear combination of original features say c1x1  c2x2  c3x3 the parameters c1 c2 c3 are called loadings
  the loading values indicate the strength of influence that each variable has on the component for example if c3 is large for pc1 then x3 the original variable greatly influences the variance explained by pc1

multicollinearity
  vif is typically calculated first to detect and address multicollinearity which can be an issue in regression models

loss of interpretability with pca
  one downside of pca is that it loses interpretability for instance if x1 is a known parameter we cannot easily quantify how changes in x1 will affect the result after applying pca

uses of pca
  dimension reduction pca helps reduce the number of variables making the data easier to handle
  prediction models pca can be used to create more efficient predictive models
  visualization pca is often used for visualizing highdimensional data by reducing it to two or three dimensions

choosing between models
  there is a choice between using y  fx where f is a matrix of features or using y  pcs principal components
  it is generally better to normalize the data before performing pca to ensure that all features are treated equally in terms of their influence on the components

mnist dataset and normalization
normalization importance
  pca on the mnist dataset highlighted the importance of normalizing the data normalization ensures that all features contribute equally to the principal components

tsne tdistributed stochastic neighbor embedding
overview tsne is a lossy transformation technique that is commonly used for dimensionality reduction and visualization particularly for highdimensional data

methodology
  tsne uses gradient descent and stochastic methods
  step 1 create a probability distribution for each observation in the dataset
  more accurately we calculate a distribution that represents the distance between each point and every other point in the dataset
  hence there are n x n distributions where n is the number of data points

why tdistribution
  a tdistribution is used because it is flatter compared to a normal distribution making it more sensitive to longdistance points this helps in maintaining the relationships between distant points

goal the goal of tsne is to minimize the kullbackleibler kl divergence which measures the difference between the original highdimensional probability distribution and the lowdimensional projection"
4,4.0,0.7445551781294232,"todays session discussed variance inflation factor vif and principal component analysis pca indepth

we first discussed vif a statistical factor to check for multicollinearity among independent variables in regression models we understood that high vif suggests high correlation between independent variables which tends to bias the coefficient estimates and decrease model interpretability the session discussed vif its interpretation and how to deal with multicollinearity the method consists of removing features individually beginning from the largest vif so that all remaining features have a vif value less than an arbitrarily selected value this both reduces the curse of dimensionality and enhances the performance of the model we also saw that vif scores would tend to form an elbowlike pattern upon plotting from which a threshold can be conveniently determined

then we discussed pca which is an unsupervised method of reducing dimension with retaining the maximum variance in the data pca is dependent on singular value decomposition svd and assists in transforming data to a new set of uncorrelated principal components principal components are orthogonal to one another thus ensuring that multicollinearity is removed we also observed how pca has the capability of dimension reduction from higher dimensions to lower dimensions as in the example where data was reduced from two dimensions to one

a major difference between vif and pca was explored vif is usually performed first to provide better interpretability because it maintains the original features which would make prediction simpler to interpret pca however converts features into principal components so it is better used for whatif analysis or delta analysis than for explicit prediction

the session also touched on various applications of pca such as dimensionality reduction predictive modeling and visualization particularly in exploratory data analysis  eda but we observed that pca is scaledependent and thus normalization becomes an essential preprocessing step

near the conclusion tsne tdistributed stochastic neighbor embedding was introduced as yet another dimension reduction method it was briefly mentioned how a gaussian and tdistribution comparison on the same dataset highlighted differences between them in picking up data variance

the session in summary illustrated how vif and pca assist with multicollinearity and dimension reduction ensuring machine learning models are interpretable and effective"
4,5.0,0.7343948418704276,in this class sir told that he will discuss about data problems if the correlation between data is not proper we use variance inflation factor vif11r2 vif 10 occurs for r2 around 08 we progressively keep eliminating the variables with higher values of vif  if we go on removing variables with high vif values we will have some variables with low vif values which have high r2 square values we need to remove some variables to eliminate the curse of dimensionality another method to handle data problems is principal component analysis principle components are orthogonal to each other we can find principal components equal to the dimension of data we transform axis so that transformed axes capture variance of data best we plot graph of two variables x1 and x2 say if the points are spread around a straight line passing through origin this straight line will be a principle component this line will capture most of the variance if we create plot  variance explained by each principle component this is known as elbow curve based on this curve we eliminate variables but the problem with this is pc1c11x1c12x2linear combination of all variables where c11c12c13 are called loadings we have to perform vif first and then use pca interpretability after doing pca yfpc using this we can do prediction or do what if analysis but it is difficult to explain the results of prediction or what if analysis with pca because pca are a mathematical representation of variables we will lose explain ability if we perform pca vif reduces the features so we do vif first and see if many features are getting eliminated then we can do pca if required pc analysis dimension reduction prediction models and visualization lets consider image data dimensionality of this data is number of pixels before pca we might need to normalise the data mnist data set is a collection of hand written numbers these have been converted into 28x28 pixels we want to train a classifier which can recognise numbers we perform pca on this data and realise that there are 784 variables but only 7080 are useful this also results in data reduction then we can do 2d and 3d plots to visualise this data pca is mathematically exact if our idea is to visualise the data effectively we need to tsne t distribution derived from gaussian distribution and it is more spread out as compared to gaussian distribution tsne is a lossy transform we loose exactness of data and gain relative closeness of data tsne is lossy transformation so use it carefully the process of tsne itself give the structure of data tsne is a machine learning algorithm that is used for dimensionality reduction and data visualisation it works by finding the similarity measures between pairs of instances in higher and lower dimensional spaces and tries to maintain the probability distribution for data samples in lower dimensions same as the probability distribution of data samples in higher dimensions tsne tdistributed stochastic neighbour encoding
5,1.0,0.7638612829835363,"sir started with the topic population vs sample sample should be good and representative probably the two should mean the same thing using the sample we predictestimate certain characteristics of the population 
analysing the entire population to understand its behaviourcharacteristics is not feasible both  time wise and money wise so we make use of representative samples
attributes
attributes associated with samples as well as population are
1	count frequency
2	mode
3	mean
4	standard deviation
5	variance and many others
operations
operations associated with samples as well as population are
1	count
2	add
3	subtract
4	multiply
5	divide and many others
then sir asked us to make a table which contains attributes and operations associated with each of the four levels of measurement

using a sample we can estimate the mean of the population
attributes associated with the population are known as parameters while those associated with the sample as statistics
we want to estimate the parameters based on the statistics
sir then showed us a scatter plot between sales and advertising expenditure and presented a case of simple linear regression simple linear regression has only one predictor
y is the response variabledependent variablelabel
x is the predictor variableindependent variablefeature
sir was then showing us a scatter plot in which it looked like the points were scattered on the surface of a circular disc then he pointed out a few things which are
1	simple linear regression is not required in such cases
2	it would not be the best possible method in such cases
3	but if someone wants to apply it then it can be applied
4	in such a case a point is a better approximation than a line this can be understood by taking any value of the feature x and then by looking at the difference in its actual value at that x and the predicted value
5	each point can be considered as a model  although a very nave one

now sir started talking about bias which is the value of the y  intercept in the equation obtained by simple linear regression he said bias is the net some of all unaccounted variables so if we develop such a model in which we have taken into account all the variables which affect the dependent variable y then we would obtain an equation which would pass through the origin 

then he said that as the size of the sample which we are using to estimate the population parameters increases the estimates become better think of this by thinking that if the entire population is the sample then the estimates would be exactly equal to the parameters if we decrease the size of this sample by some amount then the new estimates would be lesser good estimates than they were previously now keeping on decreasing the size of the sample and think about the estimates reliability it decreases prediction error would increase

then sir talked about estimation interval or confidence interval he said that finding 0 and 1 was just the start of machine learning we need to take into account the error that would be there in our estimation we have no basis to say that 0  0p
then he gave the definition of confidence interval which is that with a probability we can say that the parameter value will lie in an interval that interval is known as confidence interval typically confidence intervals are of 90 or 95 and sometimes even 99 100 confidence interval is the interval from infinity to infinity
then sir talked about how to define the best fit line now there were four options
1	minimize sum of all ei
2	minimize the sum of squares of all ei
3	minimize the sum of perpendicular distance between all pairs of xiyi and xiyibar
4	minimize the sum of absolute value of all ei
here ei  yi  yibar    
1 is rejected 4 is rejected because we dont want our solutions to be biased towards any direction so we choose 2
a professor sirmaam from iit kanpur has done 3
now refer sirs linear regression derivation and from there we see that the equation of the line that is obtained after 2 is such that the point made up of the mean of the labels and features lies on that line
now for more notes about this class refer class notes sirs lecture notes and simple linear regression proof 
it would be better if one read them from these 3 places"
5,2.0,0.754625457278052,"in todays lecture we started the discussion with population vs sample that the sample must be good and representative of the entire population we use the sample to predictestimate various attributes of the population

attributes of population
1 count frequency
2 mode
3 mean
4 median
5 standard deviation
6 variance

operations
1 count
2 add
3 subtract
4 multiply
5 divide

if these attributes are calculated from the population they are known as parameters and if they are calculated from a sample they are known as statistic
we want to estimate the parameters based on the statistics

slr  simple linear regression
it has only one predictor 

we find the best fit line for the given data in the slr model
y  b0  b1x
where y  dependent variable response variable label
             x  independent variable feature predictor

even a point can be considered as a model  although a very naive model

here b0 is called as bias as it accounts for all the other features which we didnt account for in the model
b0 and b1 are the estimates of the population parameters ie statistics
since these are calculated from the sample their confidence is very low so we need to find a confidence interval containing these estimates in which we can confidently say that the real population parameters would lie in
on increasing the interval width the confidence also increases

we have yhat  axb for each data value we define error ei  yi  yihat
now to get the best fit line we need to minimize the errors
for that taking the sum of all the individual errors wont work as the positive and negative errors might get cancelled leading to zero net error even if the individual error values are large
to address this issue we can minimize the sum of either ei or ei2 
we prefer the sum of ei2 as
1 it magnifies the error for a better fit
2 it doesnt differentiate between different directions

summationei2  summationyi  yihat2 
                                summation yi  ax b2
here we can minimize the error by making the derivative of error zero with respect to both a and b to get their values
we have closed form solution to calculate the values of a and b
we can also observe from the equations that the point with average values of features as xcoordinate and average values of labels as ycoordinate lies on the best fit line

b0 and b1 are point estimates so we need to arrive at the possible interval within which these values lie such that there is a very high chance that b0p and b1p ie the population parameters will lie within these intervals respectively"
5,3.0,0.7351325021715679,"we started by talking about population vs sample we consider a sample and determine the various attributes associated with it like mean mode median and based on the values of these attributes we predict those for our population so it is important to consider a sample that is good and represents the population well the attributes estimated for population are known as parameters while those calculated using the sample data are called as estimates statistics we talked about the different kinds of attributes that can be associated with the population data some of these include mean median mode standard deviation our goal is to estimate these parameters of the population using the calculated statistics all levels of measurement can be assigned different attributes of the data depending on the type of data they account for for example nominal level can have the attribute of mode or countfrequency but assigning the attribute of mean would be completely senseless as nominal data is qualitative
then we started discussing simple linear regression in this model we fit the data using a line a simple linear equation with just one independent variable
we can always use a point to represent any data but this choice is very naive so we instead use a simple equation like yaxb where a and b are the estimates of population parameters and x is the independent variable the y intercept b here represents the sum total of the effects of all unknown independent variables which are not considered in the equation so as we include more and more independent variables the value of b would start decreasing also depending upon the chosen sample we can have various values of a and b so every sample will have a unique value for a and b these values a and b are known as the point estimates of the actual parameters we can say with 100 confidence that the values of these point estimates do not coincide with the actual parameter values so in order to establish a certain level of certainty about the parameter values we convert the point estimates to interval estimates thereby giving a confidence interval within which the population parameters would lie with a given level of certainty as the length of confidence interval increases we become more and more confident that the estimated values resemble the parameter values now since different samples would give us different values of the estimates we need to determine the values which minimize the error error for an observation is defined as the difference between the actual value and that obtained by estimating the data set with a line so to find the values of a and b we need to minimize the sum of squares of individual errors we do this by partially differentiating the expression with respect to a and b separately and by solving the two equations for two unknowns we get the values of a and b in terms of the means and the mean of squares of the data set values after getting the values of a and b we put them in the equation to obtain the best model for the given data set we consider the squares while minimizing errors instead of simply minimizing their sum as it magnifies the errors also it prevents cancellation of the errors which can result when one point is above the line and other is below since we are getting the exact values of a and b these are known as closedform solutions
we do observe that the means of x and y all observed values satisfy the equation of the regression line so the predicted approximated line for the data values pass through their mean"
5,4.0,0.7070203884025084,"todays lecture started off with a discussion about the difference between population and sample sample is a small subset representing the entire population because gathering data for the entire population can be very difficult so the sample should be a good representative of the population and we should be able to predict the populations behaviour from the sample there are various statistical attributes of data which can be calculated some of them are frequency mean median mode std deviation variance etc if these attributes are calculated for the sample they are called as statistics whereas if they are estimated for the population they are called the parameters so our main focus with data science and ml is to estimate the parameters from the statistics 
then we moved on to simple linear regression where we have only one influencing or independent variable steering our predictions our model is basically of the form y  ax  b where x is the independent variable predictor feature y is the dependent variable response variable label and b is the bias term the bias term accounts for all the unknown variables influencing our predictions so as and when we bring more and more influencing variables into our model the bias term keeps on reducing 
one special thing that sir mentioned today was that a model can be as simple as a point even a point predictor can be a model however it may not be relevant for any practical usage and may not represent our data well 
so a and b are the statistics of the population parameters we also defined a confidence interval which is the interval of values within which the parameters lie the larger the confidence interval the more confident we can be of the parameter value lying in that interval then we also talked about minimising the error between our predictions and the actual data value at a given point we said that the error can be formulated in various ways but the most optimum way is to consider the sum of the squared errors as that is something which is least influenced by the sign and the direction of fluctuation and hence should be the best choice for minimisation upon doing minimisation we calculated the values of the statistics a and b however these were closed form solution which were basically point estimates of the parameters these estimates might not give us any confidence about the actual parameter values hence we also need to find a possible interval within which the value of the parameters can exist with a very high probability"
5,5.0,0.7038206852577834,"population and sample were further discussed upon sample is a good and representative part of the population sample is used to predictestimate the population attributes and operations such as count mode  median mean std deviation  variance  add multiply divide and subtract different level of measurement were classified on the basis of these attributes and operations population has parameters while sample has statistic we estimate parameters on the basis of these statistic linear data model was discussed upon from the sales vs advt expenditure  any model can be fitted upon any set of data but it might not be the best fit even a point can be considered a model although a naive model as sample size increases the estimation of parameters gets better and better based on th sample a slr can be fit with the equation yo1  where o is the bias in the data  as the sample size gets closer and closer to the population this bias reduces bias represents the sum of the effect of all the unknownunaccounted for variables o and 1 are estimates of the population parameters we can determine confidence intervals around the estimated parameters  the size of the confidence interval depends upon the level of confidence as the level of confidence increases the confidence interval gets wider and wider to find the best fit line we equate the sum of squares of errors of data points from the line to 0 we use ei2 because it does not depend upon the direction gives the euclidean distance rather than the manhattan distance we get in the case of ei  upon partial differentiation of the sum of squares of error we get the values of a and b of the equation yax  b
b meany  ameanx 
ameanxy  meanx  meany meanx  meanx 
o and 1are point estimates and we need to arrive at the possible interval within which their value lies such that there is a very high chance that op and 1p ie the population parameters will lie within those intervals respectively"
6,1.0,0.6698242366479389,"at the start of class sir discussed the midsem question paper sir explained the question paper and shared the approach of solving the question and problems associated with it 
first step is exploratory data analysis we first try to find the missing values in the data there were four rows with missing values one way to solve this is to drop the four rows we can justify it because we have 2371 rows another way to solve it is to fill in the missing values plotting the parameters we noticed that there is randomness in the data the histogram would show a normal distribution this suggests that even if we replace the missing data with the mean it would be ok before filling the value we check the range of value of parameters to check if we need to do standardization or normalization we find that for some parameters the range is very small while for some the range is really high we still dont have visualization for outliers for this we use the boxplots we first use box plot on all the columns to check if any column is different then we check for individual outliers surprisingly there are no outliers for the column with ailments we use a barchart or piechart we notice that heartdiseases is underrepresented one way is oversampling or under sampling as the difference is very large for the under sampled data we can conclude that the model would not be able to heartdisease properly 
heatmap is the first step in finding the correlation between the columns we notice that there is no correlation between the columns the point is that doing pairwise correlation is suicidal we need additional steps to decide if there is any correlation between columns the problem here is of multicollinearity we can use variance inflation factor analysis 

when we have two entirely different samples we try going for kde plots after creating the kde plots we notice that the distribution of each column is different for both the samples this suggests that the model fitted on one sample cannot be used on the other sample this means that both the samples come from different populations 

by the end of the class we started with a new topic the curse of dimensionality too less data chasing too much features we dont have enough data for representation of the columns this is a problem when dealing with high dimensional data this problem increases sparsity this results in loss in the discriminative power of the model the consequences are overfitting model stick to the given data and are not much reliable for this we can try to reduce the number of features we can do feature selection regularization or increase the amount of data these solutions are easier said than done we checked for variance inflation factor if a feature has a vif more than 10 then it can be expressed as a linear combination of other features we can hence eliminate the features with vif greater than 10"
6,2.0,0.6628184194994751,"in this class we mainly discussed the midsem paper sir showed us how we could have attempted the questions the first part involved performing eda on the given data set to find out the missing values and outliers the early exploratory stage involves creating scatter plots and histograms of distribution of the parameters to understand how they are distributed next to handle the missing values we can either drop them completely or we can approximate them to some value it depends on the trend in the data which can be seen through the scatter plots whether to use simple mean or some moving average in this case the data was randomly spread in the entire region hence we could use the mean value of each column to fill the missing values 
there can be mainly two types of problems problem within the column which involves missing values and the problem across the columns which involves significant differences in the values of the parameters in the two columns in such a normalization is required to ensure that a particular class is not underexpressed suppressed by the others
box plots give us idea about the outliers in this case there werent any outliers
we can visualize the categorical data using bar charts pie charts
 correlation heat maps can also be used to check for the relations between the different parameters the problem with this is that it only checks for the correlation between a feature and only one other feature at a time however it is possible that a feature depends on multiple other features and is a linear combination of those correlation maps do not provide this information hence they are sufficient but not necessary check of correlation
for solving this problem we use variance inflation factorvif it compares each feature with all other features taken together to find out the vif we express each feature as a linear combination of the others and fit a regression model on it we then find the r2 values for each of the feature vif is then calculated as 11r2 so as r2 increases vif also increases hence if r2 values is high it indicates strong correlation among the features
if vif of a feature is 10 then that feature can be expressed as a linear combination of the others hence we remove that feature column from our data
in this way we reduce the dimensionality of the data set as high dimensionality is a curse it spreads out the data more thereby making it difficult to find out important patterns from the data
the next part in the question was to check whether the derived regression model worked well for another different data set as well by plotting kde plots and analyzing the descriptive statistics of the new data set we observed that it had completely different distribution than the original data set which suggested that it belonged to a completely different population this was also evident by the looking at the values of accuracy precision recall and f1 scores of the model fitted to the new data hence the original model did not work well for the new data which belonged to a different population"
6,3.0,0.6582848233625251,in todays class we mainly discussed our midsemester test we first analyzed the data which has 24 columns of blood parameters 1 ailment column and 2371 rows providing information about the population residing in an area based on blood samples taken we identified whether they are healthy or have certain types of disease we gave an open test so that we can use tools but do not rely on them so much that they will solve the problem effectively like an expert in the test we can get solutions through multiple paths we need to take one and justify it based on the nature of the data define what kind of problem you are going to solve in the test a simple classification model was needed and we also needed to create a model and state why it is not working on the validation dataset also as per vps choice we should have made kde not histogram heading towards the solution we must report missing values either not consider them in model training or replace them with some values for this we need to know about the trenddistribution of the data for this we can plot a scatter plot of all the columns which shows the data was all over the plot as there was no time series in the question and hence randomness was there in the data the histogram should be normal we can use the mean to fix the missing values in certain cases only but not always then we need to consider the range of values across the columns do we need to normalize or standardize the data for this we plotted the histogram of all the blood parameters in them some were unimodal some were bimodal and not normal and hence we do not get anything from these but these will be helpful when comparing with new dataset we then see the descriptive statistics for the parameters and also compare with the validation data statistics and plots at the minimum we should get the data normalized between 0 and 1 hence we should have applied normalization on the datasets at least we also see the box plot of numerical data in which we see one column that was higher than others hence the need for normalization is there we also get to know about outliers through the columns and saw that there were no outliers in the columns then we analyzed the ailment column using a bar chart which can be made through a pivot table or counting in excel and also through python the heart disease population was much less compared to others and thrombocytosis was also less than another disease in the initial dataset hence the model will not identify heart disease effectively and needs more data of these in the dataset for a better model we plotted a simple correlation heatmap which tells us that the columns are not correlated much and is good for us we also discussed that treebased models are good for modelling such kinds of data then we split the dataset into test and train data 20 and 80 we applied the random forest model on the data and we saw that there was no misclassification for other diseases except heart diseases then we looked at the confusion matrix of the validation data which was not very good compared to the original dataset given then we perform the distribution analysis to see why the model performed badly on the validation dataset in that we compare the plots of the original and validation dataset which majorly shows the distributions are different for the datasets which at last states that the data are from different populations and training the model on one will not perform well on the other then we get the assessment of e3 which majorly states all have done it better and the main problem was mse loss in q3 part d then at last we saw regarding the curse of dimensionality in which we calculate vif and based on its value we can remove features
6,4.0,0.6362103558240552,"this lecture addresses practical data analysis challenges particularly those encountered when applying a trained model to new data and introduces the curse of dimensionality

data distribution mismatch the lecture begins by discussing exam performance with new data  the model trained on original data performs poorly on the new data  kernel density estimation kde plots reveal that the feature distributions differ significantly between the original and new datasets this indicates the data were sampled from different populations explaining the models poor performance

exploratory data analysis eda the lecture then moves to a solution walkthrough starting with eda  this involves

missing value handling identifying and addressing missing values using techniques like imputation mean median etc or more advanced algorithms
descriptive statistics calculating and analyzing descriptive statistics min max mean median etc for each feature
outlier detection using box plots to check for outliers in this case no outliers were found
imbalanced data analyzing the ailment target variable and finding it heavily imbalanced specifically with very low counts for heart disease this imbalance makes accurate prediction for heart disease extremely difficult techniques like oversampling and undersampling are mentioned but obtaining more data is suggested as the most effective solution
kde plots generating kde plots for all features to visualize and compare their distributions
curse of dimensionality the lecture then introduces the curse of dimensionality which arises when the number of features in a dataset increases significantly this leads to several problems

increased sparsity data points become more spread out making it harder to find meaningful patterns
increased complexity model complexity increases leading to potential overfitting
increased computational cost training and processing become more resourceintensive
distance distortion distances between data points become less meaningful as they tend to become more uniform
consequences and solutions the consequences of high dimensionality include overfitting increased computational resources and data sparsity

solutions to the curse of dimensionality include

dimensionality reduction techniques like principal component analysis pca to reduce the number of features
feature selection selecting the most relevant features
regularization techniques that penalize model complexity
increasing the amount of data more data can mitigate the effects of sparsity"
6,5.0,0.6292255155443155,"todays class was spent on revisiting the midsem exam in which we saw data insight analysis using kde plots range checks missing value imputation and box plots for outliers we determined that normalization was needed and touched upon a recommendation to oversample heart disease which was not carried out as there was extreme class imbalance
we went back to the midsem correlation heat map which at first glance seemed to have no significant relationships but this was deceptive because of multicollinearity to correct for this we calculated r values between all pairs of features and applied variance inflation factor vif to conclude that only six features were actually independent and others could be represented as linear combinations of these
the lecture also touched on performance measures the confusion matrix and ta feedback on the e3 exercise we wrapped up with a discussion of the curse of dimensionality which occurs when there are too many features compared to available data causing overfitting sparsity growth computational expense and complexity the solutions are either to grow data volume or to shrink feature dimensions"
7,1.0,0.6685623063717372,sir started with exploratory data analysis sir explained them with the example of the session summary we have been uploading in there data there were length of character in response submitted by each average length of summary submitted pivot table a tool which helps us analyse and summarise data of a column we get different values computed for a given column of data such as mean max min std dev and other such statistics this initially gives us some inferences about data which we can use to further process the data  linear regression is not valid beyond training data doing exploratory data analysis gives us ideas on what to focus on statistical summaries varies kinds of plots can be created using plots sir then showed how to analyse data of a chemical plant we have 250 columns and we have to understand nature of these columns the data has certain parameters at which the plant operates edge computing analysis data when we acquire data sensors getting more advanced that the they acquire and analyse data and give us kind of summary the pivot table automatically identifies columns like date and time when we see min and max of data we will find blank or outliers or erroneous data anomalies by using this we can remove such data we can use box plot or scatter plot to identify outliers in the report of this data analysis we will give some basic definitions of some metrics plots and inferences iqrq3q175 percentile25 percentile outliers will be q115iqr and q315iqr the factor of 15 can be changed as per our requirement some times we can identify a boundary between outliers and other data based on plots in some cases the mathematical boundary may not be suitable so we have to choose accordingly while plotting we selectively pick some parameters and them analyse them from plots based on the plots or data we need to ask questions about possible how the data is generated or what process behind that parameter we can identify relationship between variables and we can combine them together we have to ask questions about data and add those into our data when we get data we will get incorrect incomplete data we have to attribute why such an error has arises then tas gave us feedback on e2 assignment sir then said the importance of documentation
7,2.0,0.6424843211771911,"in this lecture we continued our discussion on eda we learnt how to create pivot tables in excel which summarize the entire data into a table whose entries can be chosen by us this will help us create a summary of the data and understand the various trends and patterns in the data using the pivot table entries one can then create plots to analyze the distribution pattern in the data this can be used to predict any desired value in future the example which we discussed in class was that of the average number of characters in the summary on any particular day this can be useful to predict the approximate number of characters in the summary of any day in the future
we can use various plots including box plots scatter plots histograms as well as descriptive statistics function in excel to determine the outliers in the data set any of these can be used to reach out to a conclusion regarding the outliers also it is not always correct to completely disregard the outliers as they may also reveal certain important problems patterns in the data which cannot be ignored
eda includes all these tasks of cleaning comprehending analyzing the data and extracting valuable insights from it which can be further used to build a model 
apart from this we also looked at two case studies one was regarding the measurements of numerous different parameters related to a process in chemical plant with the measurements collected on daily basis we created a pivot table which included the various parameters in the columns and the dates in the rows then we added the values of count of the total measurements made in that year the average max min of these measurements for each of the parameter we observed in some cases the min value of the parameter dropped to 0 which cannot be practically possible as we cannot have 0 pressure or temperature the problem occurred due to the missing values of these parameters on some days
the next case study was about the creation of energy using solar radiation the oil and water temperatures were recorded at different time intervals and the data was available various line plots were created to analyze the variation in these temperatures at different times of the day and further on different days
correlation maps were also made to understand the correlation between the various independent variables there were certain patterns in the temperatures from birds eye point of view however more distinct patterns were observed within these large variations when looked through a zoomed in perspective suggesting hourtohour as well as daily variations in the temperatures all these plots could be analyzed to help us understand how we can separate the noise from the signals identify the outliers and use them to predict the values of the output variable in the future"
7,3.0,0.6420286430690312,"in todays lecture sir explained how to do exploratory data analysis eda he used excel and pivot tables to do so we look at different things like mean median min max stdev histogram box plot summary statistics 
 scatter plot from the pivot table in order to get an idea about session summary data set using this we were able to get some outliers whose summarys character values were significantly higher than others then we also did the same eda for different dataset related to chemical plant where there was a data filled in 241 columns on daily basis for around 6 years there were also some missing entriesoutliers the report on this data based showed that all that 241 columns can be more or less replaced with 17 pcas at last tas discussed about our e2submissions and gave us their insights about that"
7,4.0,0.6354646326798753,"we started with pivot tables in excel we learned how pivot tables help to summarize large collections of data by grouping and totaling values we learned how to create a pivot table in excel drag and drop fields and calculate measurements like sum average count and percentages to effectively analyze data

then we moved on to other eda exploratory data analysis methods we talked about how summary statistics like mean median variance and standard deviation give an idea of the data we also covered visualization methods like histograms box plots and scatter plots to identify trends skewness and outliers

lastly we touched on how data issues like class imbalance could impact analysis when one of the classes significantly outweighs all the rest it can skew results we also did a quick overview of feature engineering and transformation which are utilized to assist in improving data quality before analysis"
7,5.0,0.6323070408171036,"class started by the introduction to pivot table we can find various parameters of a data like average of total characters sum min max and std of the total characters we analyzed the data of the submissions of summaries we checked the histogram  for the average number of characters we noticed that the majority of the portion lies in the lesser number of characters we further created a box plot to find that there were some outliers in the data 
we calculated the skewness and kurtosis and plotted a scatter plot to support the conclusion for outliers 
we get shape of distribution by the histogram other methods can help us know about the outliers 
for the data we explored the maximum submissions the minimum submission the number of times they have submitted then we went to who on which date submitted what length of submission there were several missing values in the overall sheet 
sir moved to the chemical plant data we understood how data can be calculated for every 5 min or every hour but it will just explode the data to find the inner fluctuations we need to dub up data on using pivot table we found that a lot of data is missing we went to the min max and average value of the data we found an anomaly min value for zero for a parameter to check the place in which the error occurred we cut down the observations into small chunks until we found it on plotting this data these anomalies were clearly visible we also created histogram and box plots it is a good practice to create all the images and view them at once we noticed how the plots with and without outliers look different from one another  later we proceeded to see the documentation of the data in the anomaly and questions were highlighted we do not want to exclude feature that may be useful we went on to check how many of the variables are independent we noticed that out of these hundreds of data rows we only need 17 principle component or 17 independent process to describe the entire process also we can reorganize data and bring similar observation together this makes the heatmap look better 
we also checked for the data obtained from a transformer operations the incomplete can be from sensor failure or device failure we noticed how there can be points where hypothesis can be generated and we need to decide if to accept it or reject it quality of input depends on the quality of our exploratory data analysis"
8,1.0,0.8631195402807814,"the class started off with the realisation that till now we were working with good tailored data without any issues however in real life we almost never get any such data without any problems much of the real life data has a lot of problems which need to be understood in order to solve the problems in the data and get some meaningful results for this we perform eda or exploratory data analysis on our data our data could come in various formats like text some files or some databases with large amounts of data present and we might be exposed to nontabular and nonnumeric data so we started off with a process for data mining known as crisp  dm ie cross industry standard process for data mining it has 6 steps which run cyclically which includes business understanding data understanding data preparation modelling evaluation and deployment of the model business understanding involves defining business objectives assess situations based on domain knowledge and understand the goals of the firm while also understanding the constraints also understand the success criteria and create a project plan as to how much time to allocate to different procedures 
the major steps which we were focusing on were data understanding and data preparation data understanding involves collection of data describe and explore the data and also verify the quality of data which you have collected as to whether it has some useful insights to give or not data preparation involves selection and cleaning of the data and construct the data into that which is good to work on we further moved on to eda which is a part of the data understanding step 
eda involves performing some initial investigations on the data and to gain some basic insights from it and to spot anomalies create and test hypothesis and check our assumptions eda basically uses mathematical and visual statistical tools we then went through a mind map pertaining to data problems problems can be in the dependent or independent variable or both we could have maybe no data about the dependent variable or maybe insufficient or incorrect data or maybe we could have too much data to handle each of these problems have different solutions which involve sampling or generating data based on some theory problems with independent variables involve having too many independent variables ie having multiple columns and having problems within the columns and also between columns within columns we could have missing data some distribution issues heteroscadasticity ie varying variance across the data between columns we could have too few or too many features 
eda is the first step in the data analysis pipeline where we can get some insights like how features are distributed what are the outliers etc a common eda technique involves plotting histograms and checking the distributions of each variable then we plot box plots which show us the amount of variability in the data where the central line shows us the median value and the box around it shows the variations anything outside the main boxes may be considered as outliers we also plot some correlation heat maps to understand correlations between different variables matrix plot is a 2featureeach plot plotted in a matrix formation which are used to find valuable insights but are limited to two features at a time some line plots can show some trends in the data which can be useful to make certain predictions 
we then moved on to handling missing values there are multiple types of missing data mcar  missing completely at random mar  missing at random or mnar  missing not at random mcar has completely random data points missing mar has some relationship between the missing data points which could be due to resource method of measurement etc mnar says that unobserved values are itself responsible for the data being missing now when we have identified missing data we could maybe ignore these missing points but many algorithms dont know how to handle missing data and information may be lost we could also delete all the missing values which could lead to data loss but it is the easiest to do another option could be to replace the missing values with some data statistic like the mean or the median mode could be used for categorical data sometimes instead of the data mean we could use some of the values close to the missing data point in order to fill the place we also have multi variate approaches where we observe the other columns as well in order to fill our missing value knn takes the mean of the nearest neighbour and fits the mean of the neighbouring values while mice fits a linear predictor in order to fill in values based on a trend time series data is special because we can use temporal judgement to fill the data we can use interpolation methods to fill in points or use simple moving averages as well 
we then talked about outliers which are data points that are signifcantly different from the rest of the observations some algorithms are not very sensitive to outliers while some are quite sensitive we discussed about quartiles and the inter quartile range which can be used to detect outliers standard deviation can also be used for normally distributed data where any data point beyond 3 standard deviations from the mean can be classified as an outlier for multi variate data we have dbscan which is density based spatial clustering of applications with noise where outliers are points which are not classified into any broad clusters of high density points outliers also can be dealt with in many ways like removing them from the dataset etc true outliers are data points which are extreme values but they are not erreneous for such outliers we could make a separate set out of these and deal with the normal observations and the outliers separately  for outliers we use the median and not the mean as the mean is influenced by the outliers while the median is not also for calculating the median we sort the data first so that we get the correct metric 
not all techniques are useful for all kinds of data hence domain knowledge becomes important so as to know what techniques are best for us"
8,2.0,0.7546978625773478,"in this lecture we learnt about exploratory data analysis this is the first step towards creating any machine learning model eda involves cleaning transforming understanding and analyzing the raw data through basic and primary techniques data can be analyzed to reveal patterns in it if any this can be done using either mathematical formulae or by making visual charts also many times we dont get a single data file instead we have to merge a lot of files to get a compact single data file on which we perform further analysis 
there is crispdm cross industry standard process for data mining in which there are 6 steps to be followed for building a good ml model 
the steps are
1 business understanding this involves acquiring domain knowledge clearly defining our goal and objectives and understanding the problems that are to be tackled by the model 
2 data understanding this step involves collection analysis and assessment of the relevancy and quality of data
3 data preparation it involves cleaning and transforming data into suitable form for analysis feature engineering is done to select the most relevant appropriate features
4 modelling after performing the data analysis and cleaning steps we move towards fitting a basic model to our data the model is trained based on the available data if yvalues are not available we used unsupervised learning algorithms to cluster the data and assign labels to it
5 evaluation  assessment of the model performance using various metrics and implementing changes in it if any
6 deployment implement the final model and monitor its performance to make improvements with time also effectively present the insights and results of our model
the entire process is iterative we need to repeat the steps again and again as and when required
we need to establish a certain acceptance criterion for our model for many algorithms we assume that the distribution of the features is normal however this may not be the case for every x heteroscedasticity is another problem in which the variance of the residuals is not constant and keeps varying 
in eda we have to clean the data account for the missing values and also detect the outliers and take necessary actions to handle these 
the example we discussed in class was that of clustering patients into diabetic nondiabetic by considering various features including insulin levels glucose levels age bmi etc these features in the very basic step can be analyzed using histograms which may give us some if not complete idea about the distribution we can also use correlation maps heat maps to find out the relation between these features however we cannot completely rely on these and we should take into consideration numerous other factors as well we looked upon matrix plot box plots quartile plots and some others to find the relationship in the data the box plots histograms can be used to detect outliers 
it is important to know find out what we should do with the missing values and outliers in some cases it is reasonable to ignore the entire row observation and in some others it is better to fill up the missing values all this depends on the count of missing values if there are a large number of such missing values then completely deleting those observations would leave us with merely 1015 rows which is not at all sufficient to build up a model 
for filling up the missing values we can use either mean mode or median depending on the level of measurement for example when we have nominal data we use mode because the concept of mean is not defined for nominal data 
the next step is to detect the outliers there are several methods to do so we can plot histograms box plots and find out the points that lie far beyond the region where maximum of the points lie dbscan density based spatial clustering of applications with noise is yet another method used for detecting outliers it considers a point and finds out other points around it which are in close proximity to it likewise it clusters all other points thereby forming groups the outliers remain ungrouped 
median is not affected by the outliers but the mean is hence median is preferred over mean for detecting outliers however in some cases use of mean is not justifiable as it deviates largely from the values nearby the missing values considering this as the approximate missing value would lead to larger errors 
instead of using mean or median for filling the missing values we can also use a regression model in that region and determine the missing values so even for filling the missing data and for data cleaning eda we use various models this suggests that all the steps involved in building a model from the basic raw data are iterative and are performed in loops 
by carrying out these repetitive operations we keep refining our model so as to get the best possible model for our data set"
8,3.0,0.7487674838671814,sir started with how to read confusion matrix and how to interpret it till now the data given to us was ordered and structured data but in real data will be different we have to understand the nature of data we have some tools to understand data after understanding this data we have to think and apply proper transformations so that data will be converted into a form suitable for analysis data formats can be text binary files database from internet crispdm  cross industry standard process for data mining it has six steps running cyclically 1 business understanding 2 data understanding  3 data preparation 4 modelling 5 evaluation 6 deployment currently we see data understanding and data preparation dont use deep learning networks when you are not supposed to use use the most appropriate model suitable data understanding collect initial data describe data verify data quality data preparation select data clean data construct data integrate data format data edaperforming initial investigations on the data to gain insights spot anomalies test hypothesis and check assumptions then sir showed a mind map about data problems and explained it heteroscedasticity different variance at different points of dataset then one of the teaching assistant gave explanation about eda diabetes data set was analysed the features are glucose level bmibp and insulin  the output is whether a person is diabetic or not a boxplot can help understand variability in the features and any outliers present in them feature correlations helps us filter data better matrix plot between can show more such relationships between the featurestwo at a time we have to do time based analysis how data evolves in time find any underlying trend or seasonality in the data handling missing values missing data is shown as na it is of three formats missing completely at randommcarcompletely random data points missing missing at randommarsome relationships between missing point and values in different columns  missing not at randommnar to solve this we can just leave those missing points like that we can replace the missing point with mean median or mode we can use knn we can see the nearest points near the missing point and take their average and replace the missing value micemultiple imputation by chained equations so in filling missing data we might have to use machine learning models time series data is of particular interest because of the temporal aspects that we can utilize forward and backward fill linear interpolation simple moving averages we have to understand which method is best for which type  of missing points handling outliers datapoint that significantly differs from other data detecting outliers inter quartile range standard deviation  for multivariate data dbscan density based spatial clustering of applications with noise can be used to detect outliers to visualise higher dimensional data we can use 2d tsne plotdiscussed in detail in further lectures to remove outliers data trimming and data capping
8,4.0,0.7444933660363087,"in the previous lecture we discussed how to use a confusion matrix to evaluate the quality of results its important to verify whether the rows in the matrix represent actual or predicted values as they can differ

exploratory data analysis eda and crispdm
the crispdm crossindustry standard process for data mining methodology is broken down into six steps

business understanding define the objective set goals and plan
data understanding collect describe explore and verify data quality
data preparation select clean construct integrate and format data
modeling choose techniques design tests build and assess models
evaluation review results evaluate the process and plan next steps
deployment plan deployment monitoring maintenance and report creation
challenges in data
dependent variable y common problems include incorrect data manual or automated checks insufficient data addressed by collecting more simulating or using undersamplingoversampling and too much data handled through sampling
independent variable x issues include incorrect representation solved by encoding heteroskedasticity insufficient features resolved by feature engineering too many features dimensionality reduction feature scaling problems normalized via standardization and collinearity addressed through correlation analysis
eda on pima indians diabetes dataset
box plots help identify feature variability and outliers
feature correlation shows strong relationships between features
matrix plots highlight deeper relationships between features
clusters and trends identifying trends or clusters within data can show seasonality or recurring patterns
handling missing data
types of missing data

mcar missing completely at random
mar missing at random data missing due to external factors eg sensor failure
mnar missing not at random eg missing data because it exceeds a threshold
methods to handle missing data

delete instances remove rows with missing data
impute values replace missing values with mean median etc
multivariate imputation use algorithms like knn or mice multivariate imputation by chained equations to predict missing values based on other data
for time series data missing values can be filled in with close temporal values assuming small variations

handling outliers
univariate outliers identify outliers using the interquartile range iqr defined as q3  q1 outliers fall outside the range of q1  15iqr q3  15iqr

normal distribution method points outside the normal distribution range can be considered outliers

multivariate outliers use densitybased clustering methods eg dbscan where points with few nearby neighbors are treated as outliers

handling outliers

drop outliers simply remove them from the dataset
capping apply limits to outlier values

true outliers some extreme values may not be erroneous and provide valuable insights and should be handled based on domain knowledge such outliers should be treated independently and cautiously based on their impact on the model and their relationship with the data"
8,5.0,0.7427144435312212,"todays session focus mainly on exploratory data analysis but before that we learnt that how to differentiate between actual and predicted values in confusion matrices crispdm which stands for cross industry process for data mining in these 6 steps run cyclically 
1 business understanding in which we do many steps including assessing situation assumption and constraints risk
2data understanding in these we collect initial data explore data and also verify data quality 
3 data preparation in these we select data clean data construct data integrate and format data after this we can infer which is dependent and independent variables last three steps involved modelling evaluation and deployment
exploratory data analysis is an approach used in statistics and data science to analyze and investigate data sets it involves statistical graphs and data visualization methods to visually represent the data next we see that what are the problems associated with dependent and independent variables in case of dependent variables problems such as not availabilityremedy is that use unsupervised learning and create labels incorrect insufficient data it includes very few observations or data imbalances or too much data and in  independent variables there can be problems within the column or between the column itself heteroskedastic is that different variance exists throughout the entire datasets range further ahead we explore types of missing data in the formats  
1 missing completely at random 
2 missing at random 
3 missing not at random so what do we do  1 let them be na 
2 delete all instances with missing values 
3 or replace na values with a data statistics like mean or mode mode value helps in categorical data and multivariate data imputation can also be done further we explore outliers which are the data points that differ significantly from the rest of data points most of the models does not know how to handles this but there are some models which can tackle this problem how to detect these outliers we can do these either by univariate or multivariate methods we calculate quartiles which divide a dataset into four equal parts providing insights into data distribution 
 1 first quartile q1  25 lower quartile  it represents the 25th percentile
 2 second quartile q2  it represents 50th percentile and median of the entire dataset
 3 third quartile q3  75 upper quartile
 it represents the 75th percentile meaning 75 of the data falls below this value
standard deviation methods is similar in case of multivariate we have dbscan which stands for densitybased spatial clustering of applications with noise next we see that how to handles the outliers we can do these by data trimming and data capping values that lies in the extremes arent erroneous but they are called true outliers example  stock prices can drastically shoot up or down extreme weather events one important note is that means are influenced by the outliers median are not influenced by outliers to get median data should be sorted first combination of univariate columns many results in error"
9,1.0,0.7756172523772515,"we began by addressing a doubt from the previous lecture summaries it was about the ways in which we can improve our results from the model apart from just increasing the sample size so we can also consider and try out different models before fixing one we can compare the various metrics of these models and figure out the best one if we want to stick at our original model we can fine tune use it more appropriately to improve the quality of our results one of the solutions suggested was that of grid search
after having a brief discussion about the statistics from the summary submissions of the previous classes we then moved on to understand the significance of feature engineering in improving our results from the model by using feature engineering we can create more features or destroy the already existing ones to arrive at a set of most relevant and appropriate features that significantly describe the data we can use something like polynomial regression we already have one feature x1 with us we can raise it to higher and higher powers and introduce these as additional features in the mlr model we can also have a combination of the polynomial and trigonometric functions like sinx1 we observe that as we start adding more and more of these features to our model through feature engineering the r2 value increases however the adjusted r2 decreases this is because we are adding more and more features to the model which do not significantly improve the results hence by using feature engineering and analyzing the scatter plots and histograms of errors we can arrive at the best set of features for our model which can explain much of the variation in the data
all this is a part of exploratory data analysis eda
there are two types of feature engineering techniques  forward feature engineering and backward feature engineering in forward feature engineering we keep on adding more and more features in our model by transforming the existing features in backward feature engineering we start eliminating these features one by one from the model already having many features to arrive at the best set of features the elimination is done on the basis of the pvalues of the corresponding coefficients
after this we talked about multiple model creation so in real life scenarios we first have to get the data then perform eda and preprocess it to get good data this good data can now be fed to multiple models we can simultaneously compare the metrics of all the models and decide the best one from that we can use multiple or single models to fit our data depending on the variations in it however if a single model describes the data well then it is always preferred over handling multiple models
so mainly there are two types of models parametric and nonparametric in parametric models we can control the various parameters regression coefficients and we can also perform delta analysis on it this means we can find out how much the y value would change if the x value changes by an amount say delta slr and mlr are examples of such a model however in nonparametric models like random forest we cannot perform delta analysis instead they give us better predictions than the parametric models these models are also more flexible ie they can adapt to different types of data well
one of the models which we used on the data was the artificial neural networks ann ann consists of hidden layers which have nodes these nodes map link to each and every other node in the network to form links these links have certain weights associated with them the y value is a function of these weights and the x values these weights keep changing and are recalculated as more and more features are introduced when there are more than 1 hidden layer in the network we call it the deep learning network
more the layers more capable the model becomes but at the same time we need to feed more data in the network data x values is converted into information which is represented by the weights
we fitted many models on the same data set and compared various metrics like r2 mse etc of these to determine the best model for the data apart from comparing between models we can also use the metrics like mse within the model to assess its validity
we compared the relative difference between the metrics of the test and train data for a single model to arrive at meaningful conclusions
lastly we started with classification models and discussed logistic regression in that these classification models are used on discrete data like nominal and ordinal data to classify it into various classes the y values are called labels and they determine a particular class using logistic regression we are trying to find out the line curve which separates or segregates the data into different clusters such that misclassification despite of overlapping is minimized
so every point will have feature values x1x2 associated with it based on which it is given a y value ie a label
we concluded by discussing the sigmoid function and how it can be used to identify the equation of the line further discussions to be continued in the next class"
9,2.0,0.7202991025521065,to improve the quality of results we can improve the sample by either increasing quality of sample or size of sample we can improve the method by using multiple methods and select best one also we can fine tune and properly choose parameters linear regression outcome is expressed as a linear combination of independent variables taylor seriers expansion any function can be written as a sum of powers of x so we can fit any curve by defining x2x2x3x3 and use multiple linear regression if we keep on increasing number of feature adjusted r squared value decreases after a certain point linear regression of non linear independent variables the resulting regression method is known as polynomial regression the technique of starting with all features and remove features one by one until the model performance reaches a peak is known as backward feature engineering sir gave two exercises to solve based on feature selection and polynomial regression for different datasets linear regression does not always give proper fit there are many models under supervised machine learning based on data and exploratory data analysis we choose which models to try on this data each model can handle a different type of dataset better we can use multiple models at a time also we need to fit a single good model rather than fitting multiple models to predict output we have to think in long term if we fit multiple models total cost of ownership increases linear regression and similar methods are parametric methods random forest is a non parametric model with parametric model we can do delta analysisif there is certain change in feature what would be the change in output xg boost is also a nice non parametric model it gives residuals in better normal distribution as compared to random forest neural networks are examples of parametric models most of the present day ml models are based on this if there are more than one layer it is a deep learning network but deep learning requires a lot of data when y is nominal or ordinal we use classification techniques when it is internal and ratio it becomes a regression problem we then went on to logistic regression which is a classification method regress to go back to the mean in logistic regression we try to find boundaries between groups we need to find the boundary so that the misclassification is minimised  the output in this case is a categorical variable which denotes to which group the given point belongs we need a function that will convert any variable in between 0 and 1 we use the sigmoid function for this purpose
9,3.0,0.7072793092332147,"summary
todays class discussion start on a topic of how to improve the quality of results  we can do this by 1 improving the sample like quality and size of sample 2 improving the method like using multiple methods and selecting the best one 3 fine tuning of methods or properly using the methods example  gridsearch
the new definition of linear regression is that outcome is expressed as a linear combination of independent variables
when using mlr for any datasets and if the error plot is not random follows a pattern this indicates that forcing a line to model this data results in incorrect results we need to introduce nonlinear independent variables in the system so that the multiple linear regression method can use this nonlinearity to produce the desired nonlinear ycap this method is still linear regression but it is linear regression of nonlinear independent variables the method is known as polynomial regression as polynomial terms are introduced as independent variables to handle nonlinearity in y another term is feature engineering in which we introduced additional x variables to improve the performance of ml methods there are two types of feature engineering 1 forward engineering  it starts with an empty feature set and iteratively adds one feature at a time based on their performance and 2 backward engineering  it starts with a complete set of features and removes features one by one until the model performance reaches a peak both the techniques have their advantages and disadvantages and can be used in combination to optimise the feature selection process next we talked about some nonparametric models like random forest xg boost knn etc and their features some glimpses of neural networks which is an example of parametric models are composed of neurons and links with their weights if we start increasing the layer it becomes deep learning but it needs more and more data but chances of overfitting also increases
if the difference between rsquared of train and test data is more then it is case of overfitting by comparing rsquared and mse value we can select best model next we learned about the classification when y is nominal or ordinal values if y and x are available then it is supervised learning in logistic regression which is used for classification creates boundary along the data using sigmoid function"
9,4.0,0.7053787106424385,"to improve the results you can either improve the sample or improve the method used improving the sample could mean increasing the quality of the sample or the quantity when it comes to improving the method you can either finetune or better understand the method or use multiple and select the best one based on your knowledge about the metrics one technique used to improve the method is call grid search where you form an ndimensional grid with all the possible value combinations of the n parameters of the method and then evaluate which one is the best
polynomial regression is a type of linear regression where we add additional features derived from the given features and they are respectively the features raised to an exponent eg x1  x12 x13 etc more generally we can use domain knowledge and insights gained from exploratory data analysis to engineer even better features
there are two extremes to the process of feature selection in forward feature selection we start with an empty set of selected features and then based on our knowledge keep on adding features to the dataset in the hopes of improving the results in backward feature selection we start with all the features we can think of and then start reducing them by some metric eg pvalue
in parametric methods such as linear regression we can perform delta analysis which is answering questions like how much does the output change with a slight change in a particular feature this can be somehow achieved in nonparametric models also but is much easier in parametric ones
in real life after spending 80 of the time doing something with the data that does not include fitting a model we actually need to fit many models and then decide which amongst them is the best based on various factors some of which are interpretability whether we can make sense of what the model is doing maintainability the model will have to be recreated when data drift some changes to the trend in the data that occurs with time is observed hence if one model can fit the whole data it is better and of course the understanding and feel for numbers and error metrics
a neural network is a model where we have hidden layers that are connected to input and output and amongst themselves using links that have an associated weights the output function can still be expressed as wixi more layers called deep neural networks add more flexibility  provide more degrees of freedom to the neural network but in turn it becomes extremely data hungry the latest neural network models such as chatgpt have billions of parameters and are trained on internetscale data but the task that it performs is just given a few characters  words what is the most likely next character  word
logistic regression is classification period
need to predict the boundaries that separate the different classes in case of overlap we look for minimizing the number of misclassifications define an equation for the boundary and then based on the output assign a class label one such function that can do this is the sigmoid function sa  11ea"
9,5.0,0.6918240313625248,we started our lecture with a doubt asked by a student the question was how to improve the quality of results we looked at 3 ways which can be used to do this first is improving the sample it can be done by either increasing the sample size or by increasing the quality of the sample second was to improve the method in which we use multiple methods compare them and select the best one third one was an extension of the second point which was fine tuning the method which basically means using the method properly we then began the theory by understanding the fact that multiple linear regression is not regression of linear parameters but the linear combination of any parameter we then looked at an example where the error was following some sort of sinusoidal pattern we took multiple feature like x1  x1 x2 x12  x5  sinx1 and by intuition we know that the p values of x1x2x3x4 will be high and that of x5 will be close to a zero these feature making is called as feature engineering there are two types of feature engineering  forward and backward in forward feature engineering we keep on adding feature one by one till we get a good model in case of backward feature engineering we add all the features once and remove irrelevant features one by one we then started number crunching on excel and looked at models like slr randomforest xgboost knn neural networks we looked at how the parametric models can be used for delta analysis and non parametric models like random forest cant we then had a brief on neural networks where the dependent variable is a function of features and associated weights neural network consists of nodes and links and they make up hidden layers if there are more than one hidden layer it is called as deep learning network more layers with a small sample leads to overfit we concluded our lecture with a small introduction to logistic regression which involves finding boundries for classification
10,1.0,0.6926747526836974,"recap
emphasis on
all the values in the ci are not really distinct values

what is the probability of getting da value far from the 95 ci really small
this is statistically significant

why are we worried about zero
if beta1 is statistically similar to 0 then we are in trouble  model not appropriate

if i am getting a non zero value in the ci arund 0 still not good

if i calculate a value if beta1 and it lies in the ci around 0 still similar to 0

mlr
so far dealt with one independent variable
mlr more than 1

relevant examples
photos pixels x1 x2 x3 x4
body of text need to be converted to a vector before being processed
embedding vector
if i want to deal with sales processes age earning location family size x1x2 x3 xk features

techniques to selectcreate features feature engineering

let us say you want to detect vibrations x1 which is measured in terms of hertz

we measure the frequency but it is the rate of change of frequency that we are more interested in

dx1dt  doesnt exist as measured value but we have to calculate it  this is called feature engineering

in case of slr we calculated coefficients by minimisation of eisq we had closed form solution
but now we must use numerical solutions for mlr as we dont have a closed form solution for b0 b1 b2
using
gradient descent method it is similar in principle to newton raphson
ndimensional hypersurface
capturing all equations yi  b0i  b1ix   in a matrix
y  xb  e  a very compact notation

ete is divided by 2m for convenience 

the gradient descent process
for all future ml models we learn from now on we wont have a closed form solution so we will start at a random point

solvers take an objective function and try to max or min it
f value variance by regressionvariance by random error
how do we evaluate the value of error metrics like mse rmse etc we either do it in the context of its own model or in comparison to another

where are the metrics used
sse mse optimisation
rmse mae interpretation

we need to check if errors are random even after the r2 values comes out to be good enough

which of the variables are more impactful which cause more change in the value of y when changed by the same amount

but
check if they are statistically significant p value can they be called statistically distinct from 0

so on the basis of p value we start dropping variables  feature selection
we start at the highest p value and start eliminating until we are satisfied p value  005

remember more the number of variables in consideration r2 is bound to increase"
10,2.0,0.6732205287220493,"consider a sample s1 from a population we used this data to fit a line using linear regression we have a value of a yaxb we then estimate the 95 confidence interval of a any two values which lie in the 95 confidence interval are considered statistically similar and not significant because a value outside the 95 confidence interval has very low chance of occurring it is statistically significant in linear regression we dont want value of a to be zero  so for a good model we want zero to lie outside 95 ci 
multiple linear regression deals with more than one independent variable ya0a1x1a2x2a3x3anxn    converting text into a vector is called embedding vector for salesage earnings location  x1x2x3features feature engineering is calculating useful additional features obtained by operating on existing features to draw meaningful conclusions we use mlr gradient descent to find the features matrices are used in derivations ymx1xmxlblx1emx1 eerror term here we describing all points not  the regression linein slr we describe yaxb but here yxbe where e is error termm number of observations k number of features we need to minimise the cost function  the gradient descent process 1 assume some value for bgenerally initialised to all 0s or 1s 2 using yhatxb evaluate yhat 3 calculate djdb as per the above expression by assuming some value for n  calculate new values for b using the following expression bnewboldngradj 5 repeat steps 2 to 4 until abs bnewbold reaches a threshold leveleg00001 finally we end up with a bcolumn vector which are our features f valuemsrmean square due to regressionmsemean square due to random error sse mse rmse mae are useful for comparing between two samples to qualitatively make some conclusion from data we need to calculate some other values from these in mlr based on the coefficients we can understand which features effect the output most anova analysis of variance we drop variable with highest p value more variables mean more r squared value when we drop variables with higher p values we see a decreasevery less in r squared value and f value increases we want p value to be less than 005 for the coefficients to be significant"
10,3.0,0.668998060478812,"we started the lecture by having a quick recap of the concepts from the previous class we discussed what we exactly mean by saying a given number is statistically significant a number is not statistically different from 0 we then started with a new topic multiple linear regression before starting we discussed that before performing mlr we need to convert the file data available to us in a vector form  x1x2x3 this vector contains various features of the data this process is called as embedding vector sometimes we also need to derive some new features based on the given ones which matter more are more relevant in the context so this process of transforming the already existing features or performing operations on the existing features so as to get new features is known as feature engineering
we saw that we dont get a closed form solution for the coefficient values in mlr like that in slr however the procedure needed to perform to obtain their values remains the same like slr in mlr also we try to minimize the sum of squares of errors so if we have k such independent variables features we get k such equations on which we perform numerical methods to get the solutions we also learnt about a statistic called the fstatistic it is the ratio of average variance explained by our regression model to the variance explained by errors so we want most of our variations to be explained by the regression model hence we want msr to be greater than mse which means fstatistic should be as large as possible we learnt that the error metrics that we use to assess the validity of a model are better interpreted when used to compare different models rather than using it within the same model also since rmse and mae are in the same dimensions as the data they are easier to interpret or relate as compared to other metrics like sse mse 
in any ml model we first start by assuming that the errors in the predicted and actual data values are random also for any ml model if more independent variables are available then the value of r2 increases since more variables are available to explain the variability in the data at last we saw how we can use the p values for each independent variable to assess whether it has any effect on the data if we get the pvalue 0025 for 95 interval then we can say that this particular coefficient is not statistically different from 0 hence we can ignore it and reduce the number of independent variables in the regression model we can continue this until we get only those variables whose p values are 0025 this implies that only these coefficients are significant and rest can be neglected this gives us the true actual model"
10,4.0,0.6624669970571129,"the lecture began with a recap of confidence intervals using an example we analyzed points a and b within the confidence interval range to determine whether they were truly distinct or if their values appeared by chance another point d which was outside the confidence interval was identified as statistically significant since the probability of obtaining d from the sample was very low we concluded that it was statistically different from a and b  

next we introduced embedding vectors as a way to process data we converted data into vectors that included various features allowing for feature engineering  

following this we discussed multiple linear regression mlr the objective of mlr is to express a dependent variable y as a linear combination of independent features x x x  like simple linear regression mlr aims to minimize the sum of squared errors to determine the bestfit coefficients for the features in the model  

the lecture then covered the matrices used in the derivation of mlr and the cost function which helps quantify the error the gradient descent process was then introduced to optimize the model parameters  

we also discussed the fvalue calculated as the mean square regression msr divided by the mean square error mse a higher fvalue indicates a better model fit  

next we explored the role of the pvalue in multiple linear regression if a features pvalue is greater than 005 it suggests that zero falls within the confidence interval meaning that the feature is not statistically significant based on this we learned that features with high pvalues can be dropped from the model as their presence does not significantly impact the predictions features with the highest pvalues are the least significant and should be removed to improve the models efficiency"
10,5.0,0.6432643694592783,"at first we discussed a few concepts from the previous class which were wrongly interpreted one of these was that the pvalue 95 confidence interval represents the area under the curve which lies outside the interval from both the sides so it is correct to say that for pvalue005 the coefficients are statistically significant and must be included in the regression model or we can say that the area under the curve outside the interval on either of the sides must be less than 0025 the other misconception was about the solution for the mlr model solving the equations can give us a closedform solution in theory but practically it is difficult to achieve hence we look for numerical methods to get approximate solutions which are not closed form finding such exact solutions is not practically possible as it involves calculating lots of matrix inversions these calculations are difficult as the size of these matrices is very large also many times there is multicollinearity in the independent variables this means that these independent variables are not really independent if these problems are not addressed then it can lead us to an unstable solution which may not work out for a longer time and will eventually have to be reformed to correctly predict the newly evolving data
after this we went further to discuss how exactly the procedure is carried out to analyze the data and develop models that correctly explains it whatever sample we chose should never be used completely to train our model a part of it 80 or 10 should be reserved as the test data and can be used to test our model later how much part of the data is reserved for testing depends on the size of the data if we have large data then we can keep 20 for testing and rest for training the model but if the data size is small then maybe we will have to use more than 90 to train the model if we keep on decreasing the data for training the model at a certain point it would be insufficient to train the model and this small amount of data may not as well represent the population well
 after we prepare a model based on our training data and use the test data to check whether this model correctly predicts the outcome we can get two different types of metrics
first is the training metrics this includes the error metrics associated with the training data next we have test metrics which includes the error metrics associated with the test data some metrics are relevant to both of these like r2 while some are specific to each of these so if r2 of both the sets match we can fairly conclude that the model is good enough however if we have some model which overfits the training data r21 then this model will not be good for the test data the example which we discussed in class was that a curve which passes through all the points of the training data does not fit the test data well but a best fit line for the training data is a better model for the test data we then talked about another metric ie multiple r it gives us an idea about the correlation between y and various independent variables 
adjusted r2 is another metric which we discussed adjusted r2 penalizes addition of excessive independent variables it keeps on decreasing if we add more and more variables which do not improve the model
so it gives an idea of how effective the addition of new variables is
as we predicted the y values using the mlr model and plotted these against the corresponding x values we got a best fitting curve so linear regression will not always give as a straight line it just states that the y is a linear combination of the independent variables and y may also be a non linear we had a discussion about parametric and non parametric models slr and mlr are defined as parametric models as they involve parameters  intercept beta1 beta2 these parameters have p value associated with them 
random forests and decision tree are examples of nonparametric models they dont involve any parameters and hence dont have any pvalues associated with them
so while comparing between models from these two categories we may use r2 and rmse error metrics instead of pvalues these metrics are common to both of these models at the end of the class we learnt how we can use the python libraries like scikitlearn and statsmodel to perform these regressions and give us the values of these various metrics
scikitlearn doesnt return us some metrics so we used another library which is statsmodel in which we used ols ordinary least square algorithm this gave us a variety of different metrics including kurtosis skewness aic bic tstatistics pvalues and results for few tests for normality and correlation between residuals like durbin watson omnibus test jarquebera test"
11,1.0,0.7094266968224402,for multiple linear regression the closed form solution for b exists but it might be impractical to calculate these as matrix inversions might not exist and etc if we have a sample of data we should not use the entire data for creating the ml model we need to split the data into two parts in the ratio 80training data and 20testing data this splitting has to be done randomly two sets of outcomes that we have to derive and measure training metrics and test metrics some of metrics will only be relevant to the training data but may not have any meaning for test data as we are building model using the training data there are some metrics which are suitable for this only overfit  situation when r square value of training data is much greatergenerally a difference grater than 02 than r squared value of test data the training errors maybe less but test errors will be too much in case of overfitting this is a practical tradeoff how much error we are allowing for test data and training data both these errors are important as we add more variables r square value increases this doesnot mean that the data is better fit so we introduce a adjusted r square value which is the part of variance captured by each independent variable n1 comes in the denominator of calculations involving variance and standard deviation the calculation of variance involves mean of x this is already calculated using all n variables this reduces one degree of freedom and it becomes n1 the outcome of a linear regression need not always be straight line slr and mlr are called parametric methods of model creation there are non parametric methods of model creation data drift if we create a model now the data coming after one month maybe far away from this model so we have to change our models also frequently according to the data then we shifted on to python sir explained how to do multiple linear regression in python qq plot tells us how much our data is similar to normal distribution regression model can be imported from scipy or stats model libraries smols smstatsmodel olsoptimised least square stats model gives more statistical measures and parameter than scipy low values of aic and bic are better aic and bic are to be discussed later omnibus statistics omnibus p value skewness we cant use the old p value everywhere we need to use different type of pvalue omnibus normality of distribuition jarque  bera test check the normality of residuals durbin watson test tries to asses based on value of error can we predict next error or kind of autocorrelation in the residuals of the models
11,2.0,0.6800154804778653,"the lecture began with an recap of multiple linear regression did in last lecture  where we have multiple independent variables x1 x2 x3  xn although multicollinearity is an important consideration it was noted that this would be discussed later  

next we covered the concept of training and test data emphasizing that the entire sample should not be used to train the machine learning model a typical 8020 split 80 training 20 testing was recommended as a 5050 split might lead to an unrepresentative training sample  

we then discussed two outcome sets training matrix and test matrix for the training matrix we introduced the concept of the confidence interval this led to a discussion on overfitting which occurs when training accuracy is much higher than test accuracy a graphical representation was used to illustrate this issue  

after this we performed regression statistics in excel several key terms were defined  
 multiple r the square root of r2 coefficient of determination which provides a measure of the nonlinear correlation between y and the independent variables  
 adjusted r2 the formula for adjusted r2 was explained  
 the denominator in variance formulas we examined why it is n1 instead of ndue to degrees of freedom being reduced when using the mean of x  

we clarified that linear regression does not necessarily imply a straight line but rather a linear relationship between yand x 

towards the end we transitioned to python programming for linear regression several statistical tests were covered including  
 omnibus test and its pvalue criteria
 skewness and kurtosis statistics
 durbinwatson test for autocorrelation  

additionally we discussed quantilequantile qq plots which help assess whether a dataset follows a particular distribution by dividing the sample distribution curve into sections"
11,3.0,0.6656153335473789,"class 8

explained how multicollinearity can be a problem and why we cant solve mlr by closed form solution the exact form xtx1 type form impractical inverse matrix is very hard to compute for so many features and higher number of data hence we need to shift to gradient descent for reduction of computation

splitting of data we split a sample do not consider a whole sample for training purpose around 8020 split why because we want to test the model whether it is really eefective om the unseen data or not

why not 5050 why 8020 we want the data on which the model is going to be trained to be representative enough of the data for variance or complexity capturing

two sets of outcomes training metric and test metrics training metrics include ssermse fstatistics r2 test metrics also include various operation like accuracy r2

overfitting if the r2 test is significantly lower than r2 training model is not generalized

then we tried mlr on excel showed result did the analysis various things we found out  multiple r  sqrtr2 correlation of y and all the x1x2 taken all togethernon regression statistical result resemble
adjusted r2 gives you an idea how effective the addition of new independent variable is
1sseits respective dofsstits dof  we divide by degree of freedom because we want to get the adjusted equivalent r2 if there was one independent feature dof is n1 for sst because considering the mean is known similarly for sse its nk1

moved to python  did same data analysis  residual plots how do we know this residual plot or e2 plot is okay take histogram check whether is normal distribution or not

qq plots if the errors lying on the histogram perfectly aligns with the normal curve or not
these are hardcore statistical analysis not present in sklearn  hence we use stats model

stats model give a lot of other analysis aicbic omnibus statistic omnibus pvalue jarque bera test durbin watson test"
11,4.0,0.6560945545701974,for a mlr in theory closed form solution exists but in practicality the closed form solution is not taken into consideration due to two reasons 1 finding inverse of large matrices is difficult and 2 there can be multicollinearity from a population we take sample but in order to train ml model entire sample should not be used after cleaning and examining the sample it should be divided in 8020 ratio 80 sample should be used to train the model this sample data is known as training data and the rest 20 sample should be used for testing purpose to check how well the model can predict the data this is known as testing data or unseen data then we discussed what is meant by overfit situation  case1 rtrn095  rtst075 in this case the model is able to fit the training data but is unable to predict the test data well case2 rtrn095  rtst088 in this case the model is able to fit the training data and is able to predict the test data as well in the above example case1 is overfit situation and in case2 ml model is good after this there was a discussion on different r values ie like what is meant by multiple r r adjusted r etc and what are their significance also we came to know that linear regression do not always mean fitting a straight line linear is not for straight line it is called linear regression because it uses linear combination of independent variables then we discussed what are parametric modelsslr  mlr which uses pvalue for its operations and prediction and nonparametric modelsdecision tree and random forest which rely on r rmse mse etc values for predicting outcomesthen we jumped into python discussed its two library 1 sklearnt and 2 statsmodelapi  we also discussed qq plot quantilequantile plot in order to judge the that are the residuals normally distributed or not sklearnt do not give pvalues fvalues etc so feature selection and dropping cannot happen in it where as statsmodelapi provides all the things just like excel and some even more at last we learned about olsordinary least square aic  bic lower their values better is the model omnibus statistica number arrived from some formulation of skewness  kurtosis lower its value more the residual plot is near the normality omnibus pvaluehigh pvalue suggests that residual follows normal distribution jarquebera testhigh pvalue suggests that residual follows normal distribution  durbinwatson testit was something about autocorrelation between the error terms
11,5.0,0.6549370890397888,"this lecture focuses on multiple linear regression mlr and its implementation in python  while mlr has a closedform solution its computationally expensive for large datasets therefore gradient descent is the preferred optimization method

a crucial practice in machine learning is splitting the available data into training and testing sets  a common split is 80 for training and 20 for testing performed randomly  this allows for evaluating the models performance on unseen data  two sets of evaluation metrics are generated one for the training data and one for the test data a good model exhibits strong performance on both sets  if the model performs well on the training data but poorly on the test data its a sign of overfitting conversely poor performance on both sets indicates underfitting

the multiple r metric is simply the square root of rsquared  it represents the correlation between the multiple independent variables x and the dependent variable y  adjusted rsquared is a modified version of rsquared calculated using the residual sum of squares rss and the total sum of squares tss  the formula incorporates  n1 degrees of freedom because we lose a degree of freedom for each estimated parameter adjusted rsquared penalizes the model if the rss doesnt decrease sufficiently relative to the increase in the number of predictors  it helps to prevent overfitting by considering model complexity  its important to remember that mlr doesnt always result in a straight line it can model more complex relationships

the lecture then transitions to implementing mlr in python using the sklearn library  sklearn is excellent for model building predictions and handling large datasets  however it can lack finegrained control for researchers needing detailed model analysis  to assess the models fit the distribution of residuals errors is examined using a histogram and a qq plot  ideally the residuals should be normally distributed

for more indepth analysis the statsmodels library is introduced  statsmodels provides access to a wider range of statistical metrics  one such metric is the omnibus test which combines skewness and kurtosis to assess how closely the residuals approximate a normal distribution  the jarquebera test is another normality test for residuals  a desirable outcome is a nonsignificant pvalue typically greater than 005 and a test statistic below a certain threshold eg 2 indicating that the residuals are likely normally distributed"
12,1.0,0.7472079198845185,"we started the lecture with a quick recap of the concepts from previous class including regression coefficients and discussed about the sampling distribution of mean histogram plots of the sample we learnt that whatever be the distribution of the population the sampling distribution of the sample mean will always be normally distributed we dont have the opportunity to take multiple samples we can practically take a single sample which can have multiple observations so using these statistics values we have to estimate the population parameters so while taking up a sample we assume that it is a good representative of the population
so our first step towards estimating the population parameters is to calculate the mean of sample by assuming that it is close to population mean
then we find out the sample standard distribution assuming that it is close to population standard distribution so our ultimate goal is to find out an interval around the calculated value of sample mean within which we can say with certain confidence level that our population mean would lie for our sample observations we make different categoriesbins in which we divide the data values then we plot a histogram frequency distribution graph for these bins we observe that as we start decreasing the width of the class the curve becomes smoother and smoother this curve has a gaussian normal distribution
if we divide the frequency values for various bins by the total frequency we get the probability that a certain data point will lie within that bin the area under the curve within that interval gives us the value of probability 
so when we say there is a 95 confidence interval by it we mean that if you take 100 samples then 95 out of these would lie in that interval also the area under the curve within this interval would be 095 these plots are called probability density functionpdf
if the number of observations is less than 30 then the distribution becomes a t distribution rather than a normal distribution
the normalor t distribution is centered around the mean and its tails extend to infinity on both the sides for an interval say ab any value within this is not statistically different from the mean but any value out of this interval can be said to be statistically different from the mean values it may also come from an entirely different population 
so if we want to check whether a model is truly a slr model we can check whether the coefficient of x ie a is statistically different from 0 or not if it is then we may conclude that the model is appropriate
we consider that 0 is the at the centre of the distribution and then define an interval for 95 confidence we want the value of the regression coefficient to lie outside this so as to make our regression model valid
twice the area under the curve from the point which corresponds to the regression coefficient to infinity is termed as the pvalue so for a 95 confidence interval we want our pvalue to be less than 005
we can use this for multiple linear regression as well those independent variables that have corresponding pvalues greater than 00595 confidence can be neglected in the regression expression
lastly we talked about anova analysis of variance which is a tool that tells you what is the statistic probability that at least one of the coefficients in mlr is non zero"
12,2.0,0.7294633973304252,todays class start with a question that we have only 1 sample eg 30 observations so how to estimate population mean based only on 1 sample  is it possible one interesting thing that when errors become predictive then it is not a regression model whatever the distribution of sample could be  when we take their multiple sample then we always get normal distribution lets suppose we have sample from a population then first step would be calculate the mean andassume this mean is close to the population mean if it is close to the population mean then sampling distribution of the mean is close normal distribution with mean and variance the formula for calculating standard deviation of the sampling distribution of means is standard deviation of the population root under number of observations in sample further ahead we have learnt about the confidence interval which is basically the area under curve for 95 confidence the area under curve is 095 and for the observations that doesnt lie in this region is 0025 each of side one more important thing what we learnt that if number of observations is less than 30 then we do not get normal distribution and we use t distribution which is basically a continuous probability distribution that generalizes the standard normal distribution the 95 confidence interval practically means that if we take 100 samples the mean of 95 of those samples will lies between that area in the equation yb0  b1x if b1 is statistically equal to zero then we dont get any regression if p value  005 then value of b1 is accepted and pvalue also helps in the selection of the features in the last minutes we also talked about the multiple linear regression which is a statistical technique that uses multiple independent variables to predict the value of a dependent variable and anova  which is used to compare statistically equivalence of multiple averages simultaneously the equation for mlr is yb0  b1x1  b2x2  b3x3 and if atleast on of coefficient is not zero then regression is possible fstatistic  msrmse
12,3.0,0.7130374013155768,"in todays session we went forward with more statistics
whatever the distribution of the population the sampling distribution of the mean will always be a normal distribution

problem you have only one sample eg 30 observations in real life many times it is not possible to get multiple samples we need to estimate the population mean based only on s1

for this we are assuming that the sample is representative of the population

step 1 calculate the sample mean assume this mean is close to the population mean
now sampling distribution of the mean  normalmu sigma
sxbar  sigmasqrtx  ssqrtx      
sigma population standard deviation  s sample standard deviation

step 2 calculate the sample standard deviation and assume it to be close to population standard deviation

we want to get the interval within which the population mean is likely to lie

also if the number of observations is less than 30 then instead of normal the sampling distribution follows tdistribution

95 confidence interval
if you take 100 samples the mean of 95 of those samples will lie between the lower and upper limits of the interval

then we discussed about tvalue  x  mustandard deviation

pvalue
y  b0  b1x
if b1 is statistically equal to zero then we do not have a regression
if b1 lies in the 95 confidence interval of normal distribution with mean0 and sigmasame as that of the sampling distribution of b1 then it is statistically equal to zero

pvalue should be less than 005

multiple linear regression
y  b0  b1x1  b2x2 bkxk

anova is used to compare statistical equivalence of multiple averages simultaneously
 
 fstatistic  msrmse    ideally this value should be large"
12,4.0,0.7109977799889737,"class started with a recap of previous class one of the inherent assumption of closed form of linear regression is errors are normally distributed if that is violated we have to take care of it whatever be the distribution of sample when we take multiple samples and calculate the mean of the samples we will always get normal distribution of means of samples lets suppose we have only one sample with 30 observations 
step1 calculate mean of the samplehere we assume that the calculated mean is close to the population mean 
if we have many sample all these means would form a normal distributionusrootn step2 get the sample std dev and assume it to be close to samples std deviation 
step3 calculate sigmarootn we can describe the normal distribution 
we want to get interval within which the population mean is likely to lie
using excel python and other tools we can create data with a particular distribution
after this sir went explain statistics and explained estimation of parameter using statistics then sir explained about the gaussian normal distribution if the number of observations is less than 30 we wont get a normal distribution we would get a t distribution so generally when the number of observations are less than 30 we must use t distribution we started with a sample and we are able to say with x confidence that population mean lies in an interval 95 confidence interval if you take 100 samples the mean of 95 of those samples will lie in this interval txustddev
p value we have a sample and we calculated 95 confidence interval we get a new set of observations  sample 2 and sample 3 the mean of sample 2 lies in the confidence interval whereas mean of sample 3 does not lie in confidence interval we can say that sample 2 belongs to the population and sample 3 does not belong to population 
when a0 yaxb becomes zero we cant use linear regression  we find the distribution of a we have a calculated value of a we find the confidence interval if zero lies in this confidence interval we cant have linear regression  statistically all values in confidence interval are similar p value is the area under curve between xb1 and infinity 2 p value is determined by the position of b1 for the regression to be nice p value has to be less than 005for b1 to lie outside the confidence interval p value has to be less than 005 b1 should lie outside confidence interval because we need 0 and b1 to be statistically different"
12,5.0,0.7075670869831948,"in this session the instructor discussed several key statistical concepts including error distributions standard error sample means and the central limit theorem clt a significant takeaway was that for regression models to accurately reflect the true trend in the data the errors must be normally distributed if the errors can be predicted it suggests that the model has not fully captured the underlying trend of the sample which may lead to biased results

the session outlined how to estimate the population mean from a sample which involves three main steps

1 first calculate the sample mean assuming it is close to 0 the standard error sxbar is found by dividing the populations standard deviation sigma by the square root of the sample size n
2 next compute the samples standard deviation assuming it is a good estimate of the populations standard deviation
3 finally calculate the standard deviation of the sample distribution which is essential for determining the confidence interval ci that indicates where the population mean is likely to be for example a 95 ci means that 95 out of 100 sample means will fall within that range

the session also delved into the concept of standard error which reflects how much the sample mean might differ from the actual population mean for a normal distribution the standard error is calculated as

 where  represents the sample standard deviation and  is the sample size

following this the instructor explained the differences between normal and t distributions when the population standard deviation is known the errors follow a normal distribution conversely if the population standard deviation is unknown and the sample size is less than 30 the data adheres to a t distribution the zstatistic and tstatistic serve as test statistics for normal and t distributions respectively

the concept of the pvalue was introduced with a focus on its connection to the confidence interval a low pvalue indicates strong evidence against the null hypothesis while a high pvalue suggests weak evidence the session also pointed out that if 1 in the regression equation 01 is statistically equivalent to 0 then the regression model lacks significance

additionally the session delved into multiple linear regression and anova analysis of variance as methods for comparing statistical equivalence among multiple averages the fstatistic calculated as  is utilized to evaluate the overall fit of a model in anova determining whether the group means differ significantly

in summary the session offered a thorough understanding of statistical inferences emphasizing essential concepts like standard error pvalues confidence intervals and the importance of regression models in data analysis these concepts are crucial for effective data analysis and informed decisionmaking in statistics"
13,1.0,0.6965283721013861,"in todays class we took a sample data with 100 data points and found out the best fit line using simple linear regression in ms excel we calculated the values of a and b using the data to get the regression line yhat  ax  b we plotted a scatter plot between x and y and also added the yhat points on the same graph then we calculated the error values
ei  yi  yihat and plotted the error values on a scatter plot and a histogram as well for a perfectly random data the error values should be a normal distribution bell curve on the histogram which is not the case as we observed so we say that the model failed to pick up the pattern in the data
then we used the data analysis tools for linear regression to get various information about the sample data

then we moved on to discuss what is a good model 
the model that explains most of the variations in the data
sst  summationyi  ybar2
sst  sse  ssr 
1  ssesst  ssrsst
1  r2  ssesst
r2  1  ssesst
r2  coefficient of determination
in case of simple linear regression coefficient of determination is same as the square of correlation coefficient r  
r2  r2


when drawing k representative samples si of size n from a population the means of these samples mi are expected to be similar due to their representativeness if we plot the frequency distribution of mi we typically get a histogram that resembles a normal distribution as suggested by the central limit theorem this histogram illustrates the sampling distribution of the sample mean important characteristics of this distribution include that its mean is close to the population mean and its standard deviation known as the standard error sx is connected to the population standard deviation sigma and the sample size n these characteristics highlight the advantage of using multiple smaller samples rather than relying on a single large one as the sampling distribution offers valuable insights into the populations traits"
13,2.0,0.6806661690096633,"handson linear regression analysis with excel

1 introduction

this lecture focuses on practical application of linear regression concepts using excel
we utilize the linear regression formula derived in the previous lecture to calculate the slope and intercept of the bestfit line directly within excel
the bestfit line is then plotted on the data
2 limitations of extrapolation

question can we make reliable predictions for data points outside the observed range edge points
answer no accurate extrapolation requires sufficient data points within the target region to ensure the linear regression model remains valid
3 analyzing prediction errors

we plot the error difference between predicted and actual values for each data point
observation the errors appear random
question what is the expected distribution of these random errors
4 distribution of random errors

central limit theorem when an outcome is influenced by numerous independent random factors the distribution of the observed results tends to follow a gaussian normal distribution
we create a histogram of the prediction errors to visually examine their distribution
5 automated linear regression with excel

we utilize the data analysis tool in excel to perform linear regression automatically
this provides a wealth of statistical output
6 evaluating model fit

key concept a good model effectively explains the maximum variation in the data
decomposition of variance
sst total sum of squares total variation in the dependent variable y
ssr regression sum of squares variation in y explained by the regression model
sse error sum of squares variation in y not explained by the model attributed to random error
coefficient of determination rsquared
rsquared  ssr  sst
rsquared quantifies the proportion of total variation explained by the regression model
in simple linear regression rsquared is also equal to the square of the correlation coefficient r between the independent x and dependent y variables"
13,3.0,0.6782788395511902,in todays session we plotted x and y values for a given dataset and calculated a and b of the regression line using the x and y data respectively using ms excel then we plotted the histogram graph of the errors ie actual values of y minus the predicted values of y the height of the histogram represents the no of values in the bin of a particular class width using a histogram we observed whether the data was uniform or random we saw an example of a fitted line on nonlinear data and the resulting scatter plot of the error valuesei displaying a distinct pattern stating the model has failed to pick up the inherent pattern in the data using the data analysis tool pack in excel we created a summary output of the given data showing various numbers like rsquare pvalue etc a good model explains variations in the data the measure of the total variation in the given datasetsst is equal to the sum of the total variation explained by the regression modelssr and the variation not explained by the model attributed to random errorssse and one another term that gives zero on summing it up dividing ssr with sst gives you the value of the coefficient of determinationrsquare for slr cod is equal to the square of the correlation coefficient of r a positive correlation in the data means the value of x increases and the value of y also increases while a negative correlation means if x increases then y decreases if we take a population and calculate the means of different samples we have taken the histogram of these means will follow a normal distribution the standard error of the distribution is equal to the standard deviation of the population divided by the square root of the size of the sample a random sample with a finite mean and standard deviation will approach a normal distribution as the value of n becomes sufficiently largecentral limit theorem
13,4.0,0.6738994666160675,"discussed the purpose of histograms in visualizing data distribution learned that when the output is influenced by many unknown random causes the observed distribution may follow a normal distribution highlighted that an error histogram not matching the norma distribution indicates the model fails to capture the trend in the error function

uploaded a csv file into excel for analysis performed linear regression to fit a model and plotted scatterplotsscatterplots for x vs y and for errors were created errors appeared random in the scatterplot but showed a uniform distribution in the histogram

    defined sst sum of squares total which is the total variation in the data
      sst  ssr sum of squares regression  sse sum of squares error
      sse captures variation not explained by the model random error
      ssr captures variation explained by the regression line
    introduced r coefficient of determination
      r  ssr  sst
      a higher r indicates that more variance is captured by the model
    correlation coefficient r in simple linear regression
      r  r representing the strength and direction of the relationship

explained how to derive confidence intervals using regression coefficients b and bdivided the sample into subsets calculated their means and plotted a histogram of means which resembled a normal distribution due to randomness

  explored terms like r 95 confidence interval and their interpretation in regression output discussed positive and negative correlations"
13,5.0,0.6717345397992891,"summary

1 linear regression demo in excel demonstration of linear regression in excel including visualization and interpretation of results  

2 histogram  frequency distribution introduction to histograms as graphical representations of data distribution and frequency distributions to analyze occurrences of values discussed types of distributions for truly random data  

3 regression coefficients calculation regression coefficients  and  were calculated using closedform expressions rather than iterative methods  

4 scatter plot of original data created scatter plots to visualize data points identify trends and assess relationships before applying regression models  

5 error metrics calculated key error metrics to evaluate model performance including sse sum of squared errors mse mean squared error rmse root mean squared error and mae mean absolute error  

6 example of line forcefitted on nonlinear data demonstrated an example where a linear model was applied to nonlinear data highlighting issues of poor fit  

7 scatter plot of error values visualized error values residuals noting that a good model should produce randomly distributed residuals without patterns  

8 criteria for a good model a good model is one that explains most of the variation in the data and minimizes error  

9 total variation in dataset sst explained sst total sum of squares as a measure of total data variability with components ssr sum of squares regression and sse sum of squares error the equation sst  ssr  sse  0 was discussed  

10 coefficient of determination r introduced the formula r  1  ssesst which indicates the proportion of variance explained by the model in slr  

11 standard deviation defined as a measure of dispersion and variability within the dataset relative to the mean  

12 central limit theorem clt explained how the sampling distribution of the sample mean approaches normality as sample size increases which is crucial for statistical inference"
